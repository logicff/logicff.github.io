

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/imgs/apple-touch-icon.png">
  <link rel="icon" href="/imgs/web-app-manifest-192x192.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="logicff">
  <meta name="keywords" content="">
  
    <meta name="description" content="由于毕设需要，且没怎么学习过AI这块，所以浅浅记录一下，但不会像之前记录MIT6.5840那样在实现细节上一笔带过。原因的话，对于分布式这块，好歹我有过一点分布式系统的经验以及一些知识，但AI这块，我仅仅之前半途而废地上过一点网课…  开始之前 根据个人经验，在开始作业之前，最好是先学习课程知识和相关的Python知识（如果没有的话），这样做作业会更加高效。 课程知识的话，首先是官网以及Yout">
<meta property="og:type" content="article">
<meta property="og:title" content="CS231n：Assignment1">
<meta property="og:url" content="https://logicff.github.io/2025/11/14/cs231n-1/">
<meta property="og:site_name" content="logicff">
<meta property="og:description" content="由于毕设需要，且没怎么学习过AI这块，所以浅浅记录一下，但不会像之前记录MIT6.5840那样在实现细节上一笔带过。原因的话，对于分布式这块，好歹我有过一点分布式系统的经验以及一些知识，但AI这块，我仅仅之前半途而废地上过一点网课…  开始之前 根据个人经验，在开始作业之前，最好是先学习课程知识和相关的Python知识（如果没有的话），这样做作业会更加高效。 课程知识的话，首先是官网以及Yout">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://logicff.github.io/images/cs231n/cs231n.png">
<meta property="article:published_time" content="2025-11-14T12:33:27.000Z">
<meta property="article:modified_time" content="2025-11-18T13:56:33.000Z">
<meta property="article:author" content="logicff">
<meta property="article:tag" content="计算机视觉">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://logicff.github.io/images/cs231n/cs231n.png">
  
  
  
  <title>CS231n：Assignment1 - logicff</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"logicff.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/imgs/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="logicff" type="application/atom+xml">
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>logicff</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/" target="_self">
                <i class="iconfont icon-link-fill"></i>
                <span>友链</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/atom.xml" target="_self">
                <i class="iconfont icon-rss"></i>
                <span>RSS</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/imgs/banner.gif') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="CS231n：Assignment1"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        logicff
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-11-14 20:33" pubdate>
          2025年11月14日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          5.2k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          44 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">CS231n：Assignment1</h1>
            
              <p id="updated-time" class="note note-info" style="">
                
                  
                    本文最后更新于 2025年11月18日 晚上
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <blockquote>
<p>由于毕设需要，且没怎么学习过AI这块，所以浅浅记录一下，但不会像之前记录MIT6.5840那样在实现细节上一笔带过。原因的话，对于分布式这块，好歹我有过一点分布式系统的经验以及一些知识，但AI这块，我仅仅之前半途而废地上过一点网课…</p>
</blockquote>
<h2 id="开始之前">开始之前</h2>
<p>根据个人经验，在开始作业之前，最好是先学习课程知识和相关的Python知识（如果没有的话），这样做作业会更加高效。</p>
<p>课程知识的话，首先是<a target="_blank" rel="noopener" href="https://cs231n.stanford.edu">官网</a>以及Youtube上的公开视频，B站上也会有人发。</p>
<p>相关的Python知识，在官网Schedule的2025/04/04<a target="_blank" rel="noopener" href="https://cs231n.github.io/python-numpy-tutorial/">Tutorial</a>中，也可以在<a target="_blank" rel="noopener" href="https://colab.research.google.com/github/cs231n/cs231n.github.io/blob/master/python-colab.ipynb">Colab</a>学习。最好在做作业1之前学习一下，不然对小白来说，很可能会像我一样，即使有Python基础，做个作业被Numpy什么的搞得一头雾水（比如知道该怎么实现，但一些Numpy的方法或者原理不清楚）。</p>
<p>作业的一些细节都能在官网找到，这里就不赘述了。</p>
<p>顺便分享一下我的仓库：<a target="_blank" rel="noopener" href="https://github.com/logicff/cs231n">https://github.com/logicff/cs231n</a></p>
<h2 id="q1-k-nearest-neighbor-classifier">Q1: k-Nearest Neighbor classifier</h2>
<p>这部分将在<strong>knn.ipynb</strong>中实现一个kNN图像分类器。</p>
<p>kNN分类器由两个阶段组成：</p>
<ul>
<li>在训练阶段，分类器获取训练数据并直接存储</li>
<li>在测试阶段，kNN对每个测试图像进行分类时，会将其与所有训练图像进行对比，然后采用k个最相似训练样本的标签中出现次数最多的标签作为该测试图像的标签</li>
<li>k值通过交叉验证确定</li>
</ul>
<h3 id="数据加载处理和训练阶段">数据加载、处理和训练阶段</h3>
<p>运行完初始化单元格后，导入数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Load the raw CIFAR-10 data.</span><br>cifar10_dir = <span class="hljs-string">&#x27;cs231n/datasets/cifar-10-batches-py&#x27;</span><br><br><span class="hljs-comment"># Cleaning up variables to prevent loading data multiple times (which may cause memory issue)</span><br><span class="hljs-keyword">try</span>:<br>   <span class="hljs-keyword">del</span> X_train, y_train<br>   <span class="hljs-keyword">del</span> X_test, y_test<br>   <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Clear previously loaded data.&#x27;</span>)<br><span class="hljs-keyword">except</span>:<br>   <span class="hljs-keyword">pass</span><br><br>X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)<br><br><span class="hljs-comment"># As a sanity check, we print out the size of the training and test data.</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Training data shape: &#x27;</span>, X_train.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Training labels shape: &#x27;</span>, y_train.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Test data shape: &#x27;</span>, X_test.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Test labels shape: &#x27;</span>, y_test.shape)<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># Training data shape:  (50000, 32, 32, 3)</span><br><span class="hljs-comment"># Training labels shape:  (50000,)</span><br><span class="hljs-comment"># Test data shape:  (10000, 32, 32, 3)</span><br><span class="hljs-comment"># Test labels shape:  (10000,)</span><br></code></pre></td></tr></table></figure>
<p>展示一些训练数据样本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Visualize some examples from the dataset.</span><br><span class="hljs-comment"># We show a few examples of training images from each class.</span><br>classes = [<span class="hljs-string">&#x27;plane&#x27;</span>, <span class="hljs-string">&#x27;car&#x27;</span>, <span class="hljs-string">&#x27;bird&#x27;</span>, <span class="hljs-string">&#x27;cat&#x27;</span>, <span class="hljs-string">&#x27;deer&#x27;</span>, <span class="hljs-string">&#x27;dog&#x27;</span>, <span class="hljs-string">&#x27;frog&#x27;</span>, <span class="hljs-string">&#x27;horse&#x27;</span>, <span class="hljs-string">&#x27;ship&#x27;</span>, <span class="hljs-string">&#x27;truck&#x27;</span>]<br>num_classes = <span class="hljs-built_in">len</span>(classes)<br>samples_per_class = <span class="hljs-number">7</span><br><span class="hljs-keyword">for</span> y, cls <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(classes):<br>    idxs = np.flatnonzero(y_train == y)<br>    idxs = np.random.choice(idxs, samples_per_class, replace=<span class="hljs-literal">False</span>)<br>    <span class="hljs-keyword">for</span> i, idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(idxs):<br>        plt_idx = i * num_classes + y + <span class="hljs-number">1</span><br>        plt.subplot(samples_per_class, num_classes, plt_idx)<br>        plt.imshow(X_train[idx].astype(<span class="hljs-string">&#x27;uint8&#x27;</span>))<br>        plt.axis(<span class="hljs-string">&#x27;off&#x27;</span>)<br>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span>:<br>            plt.title(cls)<br>plt.show()<br></code></pre></td></tr></table></figure>
<p>为了测试效率，仅取训练集前5000个作为当前训练集，取测试集前500个作为当前测试集，并将图像展平为一个行向量，展平后的列数为32×32×3=3072列。：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Subsample the data for more efficient code execution in this exercise</span><br>num_training = <span class="hljs-number">5000</span><br>mask = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(num_training))<br>X_train = X_train[mask]<br>y_train = y_train[mask]<br><br>num_test = <span class="hljs-number">500</span><br>mask = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(num_test))<br>X_test = X_test[mask]<br>y_test = y_test[mask]<br><br><span class="hljs-comment"># Reshape the image data into rows</span><br>X_train = np.reshape(X_train, (X_train.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>))<br>X_test = np.reshape(X_test, (X_test.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>))<br><span class="hljs-built_in">print</span>(X_train.shape, X_test.shape)<br><br><span class="hljs-comment"># Output</span><br><span class="hljs-comment"># (5000, 3072) (500, 3072)</span><br></code></pre></td></tr></table></figure>
<p>导入kNN分类器，并存储训练数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> cs231n.classifiers <span class="hljs-keyword">import</span> KNearestNeighbor<br><br><span class="hljs-comment"># Create a kNN classifier instance.</span><br><span class="hljs-comment"># Remember that training a kNN classifier is a noop:</span><br><span class="hljs-comment"># the Classifier simply remembers the data and does no further processing</span><br>classifier = KNearestNeighbor()<br>classifier.train(X_train, y_train)<br></code></pre></td></tr></table></figure>
<h3 id="测试阶段">测试阶段</h3>
<p>计算每张测试图片与每张训练图片的距离：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Open cs231n/classifiers/k_nearest_neighbor.py and implement</span><br><span class="hljs-comment"># compute_distances_two_loops.</span><br><br><span class="hljs-comment"># Test your implementation:</span><br>dists = classifier.compute_distances_two_loops(X_test)<br><span class="hljs-built_in">print</span>(dists.shape)<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># (500, 5000)</span><br></code></pre></td></tr></table></figure>
<p>在<code>cs231n/classifiers/k_nearest_neighbor.py</code>中实现<code>compute_distances_two_loops</code>，直接套公式即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># In cs231n/classifiers/k_nearest_neighbor.py</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_distances_two_loops</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Compute the distance between each test point in X and each training point</span><br><span class="hljs-string">        in self.X_train using a nested loop over both the training data and the</span><br><span class="hljs-string">        test data.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - X: A numpy array of shape (num_test, D) containing test data.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span><br><span class="hljs-string">          is the Euclidean distance between the ith test point and the jth training</span><br><span class="hljs-string">          point.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        num_test = X.shape[<span class="hljs-number">0</span>]<br>        num_train = <span class="hljs-variable language_">self</span>.X_train.shape[<span class="hljs-number">0</span>]<br>        dists = np.zeros((num_test, num_train))<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_test):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_train):<br>                <span class="hljs-comment">#####################################################################</span><br>                <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                             #</span><br>                <span class="hljs-comment"># Compute the l2 distance between the ith test point and the jth    #</span><br>                <span class="hljs-comment"># training point, and store the result in dists[i, j]. You should   #</span><br>                <span class="hljs-comment"># not use a loop over dimension, nor use np.linalg.norm().          #</span><br>                <span class="hljs-comment">#####################################################################</span><br>                <span class="hljs-comment"># L2 distance: d2(I1, I2) = \sqrt&#123;\sum&#123;\square&#123;I^p_1 - I^p_2&#125;&#125;&#125;</span><br>                dists[i, j] = np.sqrt(np.<span class="hljs-built_in">sum</span>(np.square(X[i] - <span class="hljs-variable language_">self</span>.X_train[j])))<br>        <span class="hljs-keyword">return</span> dists<br></code></pre></td></tr></table></figure>
<p>回到knn.ipynb，可视化得到的<code>dists</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># We can visualize the distance matrix: each row is a single test example and</span><br><span class="hljs-comment"># its distances to training examples</span><br>plt.imshow(dists, interpolation=<span class="hljs-string">&#x27;none&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>
<figure>
<img src="/images/cs231n/dists-visual.png" srcset="/imgs/loading.gif" lazyload alt="visualized distance matrix" /><figcaption>visualized distance matrix</figcaption>
</figure>
<p><strong>Inline Question 1</strong></p>
<p>Notice the structured patterns in the distance matrix, where some rows or columns are visibly brighter. (Note that with the default color scheme black indicates low distances while white indicates high distances.)</p>
<ul>
<li>What in the data is the cause behind the distinctly bright rows?</li>
<li>What causes the columns?</li>
</ul>
<p><span class="math inline">$\color{blue}{\textit Your Answer:}$</span></p>
<ul>
<li><p>Bright rows correspond to test samples that are dissimilar to all training samples. This happens because these test samples may be have high noise, or belong to classes with weak representation in the training set—leading to large distances to all stored training samples.</p></li>
<li><p>Bright columns correspond to training samples that are dissimilar to all test samples.</p></li>
</ul>
<p>第二个任务点，预测标签，输入一个距离矩阵，和最近邻居数量k，输出测试集的预测标签。先在<code>cs231n/classifiers/k_nearest_neighbor.py</code>中实现<code>predict_labels</code>，按照注释中的提示做即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># In cs231n/classifiers/k_nearest_neighbor.py</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict_labels</span>(<span class="hljs-params">self, dists, k=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Given a matrix of distances between test points and training points,</span><br><span class="hljs-string">        predict a label for each test point.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span><br><span class="hljs-string">          gives the distance betwen the ith test point and the jth training point.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">        - y: A numpy array of shape (num_test,) containing predicted labels for the</span><br><span class="hljs-string">          test data, where y[i] is the predicted label for the test point X[i].</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        num_test = dists.shape[<span class="hljs-number">0</span>]<br>        y_pred = np.zeros(num_test)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_test):<br>            <span class="hljs-comment"># A list of length k storing the labels of the k nearest neighbors to</span><br>            <span class="hljs-comment"># the ith test point.</span><br>            closest_y = []<br>            <span class="hljs-comment">#########################################################################</span><br>            <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                 #</span><br>            <span class="hljs-comment"># Use the distance matrix to find the k nearest neighbors of the ith    #</span><br>            <span class="hljs-comment"># testing point, and use self.y_train to find the labels of these       #</span><br>            <span class="hljs-comment"># neighbors. Store these labels in closest_y.                           #</span><br>            <span class="hljs-comment"># Hint: Look up the function numpy.argsort.                             #</span><br>            <span class="hljs-comment">#########################################################################</span><br>            k_nearest_idxs = np.argsort(dists[i])[:k]<br>            closest_y = <span class="hljs-variable language_">self</span>.y_train[k_nearest_idxs]<br><br><br>            <span class="hljs-comment">#########################################################################</span><br>            <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                 #</span><br>            <span class="hljs-comment"># Now that you have found the labels of the k nearest neighbors, you    #</span><br>            <span class="hljs-comment"># need to find the most common label in the list closest_y of labels.   #</span><br>            <span class="hljs-comment"># Store this label in y_pred[i]. Break ties by choosing the smaller     #</span><br>            <span class="hljs-comment"># label.                                                                #</span><br>            <span class="hljs-comment">#########################################################################</span><br>            counts = np.bincount(closest_y)<br>            y_pred[i] = np.argmax(counts)<br><br><br>        <span class="hljs-keyword">return</span> y_pred<br></code></pre></td></tr></table></figure>
<p>使用k=1进行预测，查看预测准确率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Now implement the function predict_labels and run the code below:</span><br><span class="hljs-comment"># We use k = 1 (which is Nearest Neighbor).</span><br>y_test_pred = classifier.predict_labels(dists, k=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># Compute and print the fraction of correctly predicted examples</span><br>num_correct = np.<span class="hljs-built_in">sum</span>(y_test_pred == y_test)<br>accuracy = <span class="hljs-built_in">float</span>(num_correct) / num_test<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Got %d / %d correct =&gt; accuracy: %f&#x27;</span> % (num_correct, num_test, accuracy))<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># Got 137 / 500 correct =&gt; accuracy: 0.274000</span><br></code></pre></td></tr></table></figure>
<p>再试试k=5：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">y_test_pred = classifier.predict_labels(dists, k=<span class="hljs-number">5</span>)<br>num_correct = np.<span class="hljs-built_in">sum</span>(y_test_pred == y_test)<br>accuracy = <span class="hljs-built_in">float</span>(num_correct) / num_test<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Got %d / %d correct =&gt; accuracy: %f&#x27;</span> % (num_correct, num_test, accuracy))<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># Got 139 / 500 correct =&gt; accuracy: 0.278000</span><br></code></pre></td></tr></table></figure>
<p>可以看到k=5的表现要比k=1稍微好一点。</p>
<p><strong>Inline Question 2</strong></p>
<p>We can also use other distance metrics such as L1 distance. For pixel values <span class="math inline"><em>p</em><sub><em>i</em><em>j</em></sub><sup>(<em>k</em>)</sup></span> at location <span class="math inline">(<em>i</em>, <em>j</em>)</span> of some image <span class="math inline"><em>I</em><sub><em>k</em></sub></span>,</p>
<p>the mean <span class="math inline"><em>μ</em></span> across all pixels over all images is <br /><span class="math display">$$\mu=\frac{1}{nhw}\sum_{k=1}^n\sum_{i=1}^{h}\sum_{j=1}^{w}p_{ij}^{(k)}$$</span><br /> And the pixel-wise mean <span class="math inline"><em>μ</em><sub><em>i</em><em>j</em></sub></span> across all images is <br /><span class="math display">$$\mu_{ij}=\frac{1}{n}\sum_{k=1}^np_{ij}^{(k)}.$$</span><br /> The general standard deviation <span class="math inline"><em>σ</em></span> and pixel-wise standard deviation <span class="math inline"><em>σ</em><sub><em>i</em><em>j</em></sub></span> is defined similarly.</p>
<p>Which of the following preprocessing steps will not change the performance of a Nearest Neighbor classifier that uses L1 distance? Select all that apply. To clarify, both training and test examples are preprocessed in the same way.</p>
<ol type="1">
<li>Subtracting the mean <span class="math inline"><em>μ</em></span> (<span class="math inline"><em>p̃</em><sub><em>i</em><em>j</em></sub><sup>(<em>k</em>)</sup> = <em>p</em><sub><em>i</em><em>j</em></sub><sup>(<em>k</em>)</sup> − <em>μ</em></span>.)</li>
<li>Subtracting the per pixel mean <span class="math inline"><em>μ</em><sub><em>i</em><em>j</em></sub></span> (<span class="math inline"><em>p̃</em><sub><em>i</em><em>j</em></sub><sup>(<em>k</em>)</sup> = <em>p</em><sub><em>i</em><em>j</em></sub><sup>(<em>k</em>)</sup> − <em>μ</em><sub><em>i</em><em>j</em></sub></span>.)</li>
<li>Subtracting the mean <span class="math inline"><em>μ</em></span> and dividing by the standard deviation <span class="math inline"><em>σ</em></span>.</li>
<li>Subtracting the pixel-wise mean <span class="math inline"><em>μ</em><sub><em>i</em><em>j</em></sub></span> and dividing by the pixel-wise standard deviation <span class="math inline"><em>σ</em><sub><em>i</em><em>j</em></sub></span>.</li>
<li>Rotating the coordinate axes of the data, which means rotating all the images by the same angle. Empty regions in the image caused by rotation are padded with a same pixel value and no interpolation is performed.</li>
</ol>
<p><span class="math inline">$\color{blue}{\textit Your Answer:}$</span> 1, 2, 3, 5</p>
<p><span class="math inline">$\color{blue}{\textit Your Explanation:}$</span> The core principle is that preprocessing must preserve the relative magnitudes of L1 distances between samples. Only 4 breaks it.</p>
<p>现在尝试在一个循环内完成<code>dists</code>的计算，即每次循环直接计算一个测试图像与所有训练图像的距离，可以利用Numpy的广播机制（开始提到的<a target="_blank" rel="noopener" href="https://cs231n.github.io/python-numpy-tutorial/">Tutorial</a>中有讲）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># In cs231n/classifiers/k_nearest_neighbor.py</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_distances_one_loop</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Compute the distance between each test point in X and each training point</span><br><span class="hljs-string">        in self.X_train using a single loop over the test data.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Input / Output: Same as compute_distances_two_loops</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        num_test = X.shape[<span class="hljs-number">0</span>]<br>        num_train = <span class="hljs-variable language_">self</span>.X_train.shape[<span class="hljs-number">0</span>]<br>        dists = np.zeros((num_test, num_train))<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_test):<br>            <span class="hljs-comment">#######################################################################</span><br>            <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                               #</span><br>            <span class="hljs-comment"># Compute the l2 distance between the ith test point and all training #</span><br>            <span class="hljs-comment"># points, and store the result in dists[i, :].                        #</span><br>            <span class="hljs-comment"># Do not use np.linalg.norm().                                        #</span><br>            <span class="hljs-comment">#######################################################################</span><br>            dists[i] = np.sqrt(np.<span class="hljs-built_in">sum</span>(np.square(X[i] - <span class="hljs-variable language_">self</span>.X_train), axis=<span class="hljs-number">1</span>))<br>        <span class="hljs-keyword">return</span> dists<br></code></pre></td></tr></table></figure>
<p>测试并检查正确性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Now lets speed up distance matrix computation by using partial vectorization</span><br><span class="hljs-comment"># with one loop. Implement the function compute_distances_one_loop and run the</span><br><span class="hljs-comment"># code below:</span><br>dists_one = classifier.compute_distances_one_loop(X_test)<br><br><span class="hljs-comment"># To ensure that our vectorized implementation is correct, we make sure that it</span><br><span class="hljs-comment"># agrees with the naive implementation. There are many ways to decide whether</span><br><span class="hljs-comment"># two matrices are similar; one of the simplest is the Frobenius norm. In case</span><br><span class="hljs-comment"># you haven&#x27;t seen it before, the Frobenius norm of two matrices is the square</span><br><span class="hljs-comment"># root of the squared sum of differences of all elements; in other words, reshape</span><br><span class="hljs-comment"># the matrices into vectors and compute the Euclidean distance between them.</span><br>difference = np.linalg.norm(dists - dists_one, <span class="hljs-built_in">ord</span>=<span class="hljs-string">&#x27;fro&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;One loop difference was: %f&#x27;</span> % (difference, ))<br><span class="hljs-keyword">if</span> difference &lt; <span class="hljs-number">0.001</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Good! The distance matrices are the same&#x27;</span>)<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Uh-oh! The distance matrices are different&#x27;</span>)<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># One loop difference was: 0.000000</span><br><span class="hljs-comment"># Good! The distance matrices are the same</span><br></code></pre></td></tr></table></figure>
<p>接下来，再尝试无循环计算<code>dists</code>，这次要和之前的two_loop、one_loop有本质上的不同。</p>
<p>尽管one_loop借助了Numpy的广播机制，但在计算量和计算效率上和two_loop差别不大，可以说one_loop和two_loop没有本质上的差别。</p>
<p>首先，我们看距离公式： <br /><span class="math display"><em>D</em><sub><em>i</em>, <em>j</em></sub> = (<em>t</em><em>e</em><em>s</em><em>t</em><sub><em>i</em></sub> − <em>t</em><em>r</em><em>a</em><em>i</em><em>n</em><sub><em>j</em></sub>)<sup>2</sup> = <em>t</em><em>e</em><em>s</em><em>t</em><sub><em>i</em></sub><sup>2</sup> + <em>t</em><em>r</em><em>a</em><em>i</em><em>n</em><sub><em>j</em></sub><sup>2</sup> − 2<em>t</em><em>e</em><em>s</em><em>t</em><sub><em>i</em></sub> * <em>t</em><em>r</em><em>a</em><em>i</em><em>n</em><sub><em>j</em></sub></span><br /> 展开之前，想要不用循环的话，可以借助Numpy的广播机制，如<code>dists = np.sqrt(np.sum(np.square(X[:, np.newaxis, :] - self.X_train), axis=2))</code>，或者<code>dists = np.sqrt(np.sum(np.square(X[:, np.newaxis, :] - self.X_train[np.newaxis, :, :]), axis=2))</code>，但是这个方法，一是和之前的one_loop和two_loop没有本质上的差别（除非有底层优化），二是内存占用也是相当大的（拿作业的数据来说，<code>X[:, np.newaxis, :] - self.X_train[np.newaxis, :, :]</code>将得到一个包含num_test×num_train×3072个元素的大数组/矩阵）。</p>
<p>展开之后，两个平方项可以不用多次计算，分别计算后保存，后续复用这些结果即可，有点像动态规划的思想，而展开前的平方项由于同时依赖test_i和train_j，就没法这么做了。除此之外，test_i * train_j则可借助矩阵乘法实现，虽然计算量没变，但矩阵乘法是有底层优化的，运行速度很快。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># In cs231n/classifiers/k_nearest_neighbor.py</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_distances_no_loops</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Compute the distance between each test point in X and each training point</span><br><span class="hljs-string">        in self.X_train using no explicit loops.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Input / Output: Same as compute_distances_two_loops</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        num_test = X.shape[<span class="hljs-number">0</span>]<br>        num_train = <span class="hljs-variable language_">self</span>.X_train.shape[<span class="hljs-number">0</span>]<br>        dists = np.zeros((num_test, num_train))<br>        <span class="hljs-comment">#########################################################################</span><br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                 #</span><br>        <span class="hljs-comment"># Compute the l2 distance between all test points and all training      #</span><br>        <span class="hljs-comment"># points without using any explicit loops, and store the result in      #</span><br>        <span class="hljs-comment"># dists.                                                                #</span><br>        <span class="hljs-comment">#                                                                       #</span><br>        <span class="hljs-comment"># You should implement this function using only basic array operations; #</span><br>        <span class="hljs-comment"># in particular you should not use functions from scipy,                #</span><br>        <span class="hljs-comment"># nor use np.linalg.norm().                                             #</span><br>        <span class="hljs-comment">#                                                                       #</span><br>        <span class="hljs-comment"># HINT: Try to formulate the l2 distance using matrix multiplication    #</span><br>        <span class="hljs-comment">#       and two broadcast sums.                                         #</span><br>        <span class="hljs-comment">#########################################################################</span><br>        sum_x2 = np.reshape(np.<span class="hljs-built_in">sum</span>(np.square(X), axis=<span class="hljs-number">1</span>), (num_test, <span class="hljs-number">1</span>))<br>        sum_xt2 = np.reshape(np.<span class="hljs-built_in">sum</span>(np.square(<span class="hljs-variable language_">self</span>.X_train), axis=<span class="hljs-number">1</span>), (<span class="hljs-number">1</span>, num_train))<br>        matmul_xxt = np.dot(X, <span class="hljs-variable language_">self</span>.X_train.T)<br>        dists = np.sqrt(sum_x2 + sum_xt2 - <span class="hljs-number">2</span> * matmul_xxt)<br>        <span class="hljs-keyword">return</span> dists<br></code></pre></td></tr></table></figure>
<p>测试并检查正确性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Now implement the fully vectorized version inside compute_distances_no_loops</span><br><span class="hljs-comment"># and run the code</span><br>dists_two = classifier.compute_distances_no_loops(X_test)<br><br><span class="hljs-comment"># check that the distance matrix agrees with the one we computed before:</span><br>difference = np.linalg.norm(dists - dists_two, <span class="hljs-built_in">ord</span>=<span class="hljs-string">&#x27;fro&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;No loop difference was: %f&#x27;</span> % (difference, ))<br><span class="hljs-keyword">if</span> difference &lt; <span class="hljs-number">0.001</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Good! The distance matrices are the same&#x27;</span>)<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Uh-oh! The distance matrices are different&#x27;</span>)<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># No loop difference was: 0.000000</span><br><span class="hljs-comment"># Good! The distance matrices are the same</span><br></code></pre></td></tr></table></figure>
<p>三种方法的运行时间对比：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Let&#x27;s compare how fast the implementations are</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">time_function</span>(<span class="hljs-params">f, *args</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Call a function f with args and return the time (in seconds) that it took to execute.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">import</span> time<br>    tic = time.time()<br>    f(*args)<br>    toc = time.time()<br>    <span class="hljs-keyword">return</span> toc - tic<br><br>two_loop_time = time_function(classifier.compute_distances_two_loops, X_test)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Two loop version took %f seconds&#x27;</span> % two_loop_time)<br><br>one_loop_time = time_function(classifier.compute_distances_one_loop, X_test)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;One loop version took %f seconds&#x27;</span> % one_loop_time)<br><br>no_loop_time = time_function(classifier.compute_distances_no_loops, X_test)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;No loop version took %f seconds&#x27;</span> % no_loop_time)<br><br><span class="hljs-comment"># You should see significantly faster performance with the fully vectorized implementation!</span><br><br><span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> depending on what machine you&#x27;re using,</span><br><span class="hljs-comment"># you might not see a speedup when you go from two loops to one loop,</span><br><span class="hljs-comment"># and might even see a slow-down.</span><br><br><span class="hljs-comment"># my Output:</span><br><span class="hljs-comment"># Two loop version took 39.707868 seconds</span><br><span class="hljs-comment"># One loop version took 61.474561 seconds</span><br><span class="hljs-comment"># No loop version took 0.530087 seconds</span><br><span class="hljs-comment"># 说明：运行到one_loop时和Colab连接断开，一会后才继续恢复执行，所以时间有点离谱</span><br><span class="hljs-comment"># 不过跑前面的的代码块的时候one_loop的时间大概45秒左右（没记错的话）。</span><br></code></pre></td></tr></table></figure>
<h3 id="交叉验证">交叉验证</h3>
<p>我们已经实现了kNN分类器，但目前的k=5是随意设定的。我们将通过交叉验证取得最好的超参数值（k值）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python">num_folds = <span class="hljs-number">5</span><br>k_choices = [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">8</span>, <span class="hljs-number">10</span>, <span class="hljs-number">12</span>, <span class="hljs-number">15</span>, <span class="hljs-number">20</span>, <span class="hljs-number">50</span>, <span class="hljs-number">100</span>]<br><br>X_train_folds = []<br>y_train_folds = []<br><span class="hljs-comment">################################################################################</span><br><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                        #</span><br><span class="hljs-comment"># Split up the training data into folds. After splitting, X_train_folds and    #</span><br><span class="hljs-comment"># y_train_folds should each be lists of length num_folds, where                #</span><br><span class="hljs-comment"># y_train_folds[i] is the label vector for the points in X_train_folds[i].     #</span><br><span class="hljs-comment"># Hint: Look up the numpy array_split function.                                #</span><br><span class="hljs-comment">################################################################################</span><br>X_train_folds = np.array_split(X_train, num_folds)<br>y_train_folds = np.array_split(y_train, num_folds)<br><br><span class="hljs-comment"># A dictionary holding the accuracies for different values of k that we find</span><br><span class="hljs-comment"># when running cross-validation. After running cross-validation,</span><br><span class="hljs-comment"># k_to_accuracies[k] should be a list of length num_folds giving the different</span><br><span class="hljs-comment"># accuracy values that we found when using that value of k.</span><br>k_to_accuracies = &#123;&#125;<br><br><br><span class="hljs-comment">################################################################################</span><br><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                        #</span><br><span class="hljs-comment"># Perform k-fold cross validation to find the best value of k. For each        #</span><br><span class="hljs-comment"># possible value of k, run the k-nearest-neighbor algorithm num_folds times,   #</span><br><span class="hljs-comment"># where in each case you use all but one of the folds as training data and the #</span><br><span class="hljs-comment"># last fold as a validation set. Store the accuracies for all fold and all     #</span><br><span class="hljs-comment"># values of k in the k_to_accuracies dictionary.                               #</span><br><span class="hljs-comment">################################################################################</span><br><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> k_choices:<br>  k_to_accuracies[k] = []<br>  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_folds):<br>    train_X = np.concatenate([X_train_folds[j] <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_folds) <span class="hljs-keyword">if</span> j != i])<br>    train_y = np.concatenate([y_train_folds[j] <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_folds) <span class="hljs-keyword">if</span> j != i])<br>    test_X = X_train_folds[i]<br>    test_y = y_train_folds[i]<br>    classifier = KNearestNeighbor()<br>    classifier.train(train_X, train_y)<br>    test_y_pred = classifier.predict(X=test_X, k=k, num_loops=<span class="hljs-number">0</span>)<br>    num_correct = np.<span class="hljs-built_in">sum</span>(test_y_pred == test_y)<br>    accuracy = <span class="hljs-built_in">float</span>(num_correct) / <span class="hljs-built_in">len</span>(test_y)<br>    k_to_accuracies[k].append(accuracy)<br><br><span class="hljs-comment"># Print out the computed accuracies</span><br><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(k_to_accuracies):<br>    <span class="hljs-keyword">for</span> accuracy <span class="hljs-keyword">in</span> k_to_accuracies[k]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;k = %d, accuracy = %f&#x27;</span> % (k, accuracy))<br></code></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs output">k = 1, accuracy = 0.263000<br>k = 1, accuracy = 0.257000<br>k = 1, accuracy = 0.264000<br>k = 1, accuracy = 0.278000<br>k = 1, accuracy = 0.266000<br>k = 3, accuracy = 0.239000<br>k = 3, accuracy = 0.249000<br>k = 3, accuracy = 0.240000<br>k = 3, accuracy = 0.266000<br>k = 3, accuracy = 0.254000<br>k = 5, accuracy = 0.248000<br>k = 5, accuracy = 0.266000<br>k = 5, accuracy = 0.280000<br>k = 5, accuracy = 0.292000<br>k = 5, accuracy = 0.280000<br>k = 8, accuracy = 0.262000<br>k = 8, accuracy = 0.282000<br>k = 8, accuracy = 0.273000<br>k = 8, accuracy = 0.290000<br>k = 8, accuracy = 0.273000<br>k = 10, accuracy = 0.265000<br>k = 10, accuracy = 0.296000<br>k = 10, accuracy = 0.276000<br>k = 10, accuracy = 0.284000<br>k = 10, accuracy = 0.280000<br>k = 12, accuracy = 0.260000<br>k = 12, accuracy = 0.295000<br>k = 12, accuracy = 0.279000<br>k = 12, accuracy = 0.283000<br>k = 12, accuracy = 0.280000<br>k = 15, accuracy = 0.252000<br>k = 15, accuracy = 0.289000<br>k = 15, accuracy = 0.278000<br>k = 15, accuracy = 0.282000<br>k = 15, accuracy = 0.274000<br>k = 20, accuracy = 0.270000<br>k = 20, accuracy = 0.279000<br>k = 20, accuracy = 0.279000<br>k = 20, accuracy = 0.282000<br>k = 20, accuracy = 0.285000<br>k = 50, accuracy = 0.271000<br>k = 50, accuracy = 0.288000<br>k = 50, accuracy = 0.278000<br>k = 50, accuracy = 0.269000<br>k = 50, accuracy = 0.266000<br>k = 100, accuracy = 0.256000<br>k = 100, accuracy = 0.270000<br>k = 100, accuracy = 0.263000<br>k = 100, accuracy = 0.256000<br>k = 100, accuracy = 0.263000<br></code></pre></td></tr></table></figure>
<p>可视化一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># plot the raw observations</span><br><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> k_choices:<br>    accuracies = k_to_accuracies[k]<br>    plt.scatter([k] * <span class="hljs-built_in">len</span>(accuracies), accuracies)<br><br><span class="hljs-comment"># plot the trend line with error bars that correspond to standard deviation</span><br>accuracies_mean = np.array([np.mean(v) <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(k_to_accuracies.items())])<br>accuracies_std = np.array([np.std(v) <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(k_to_accuracies.items())])<br>plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)<br>plt.title(<span class="hljs-string">&#x27;Cross-validation on k&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;k&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Cross-validation accuracy&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>
<figure>
<img src="/images/cs231n/knn-cross-val.png" srcset="/imgs/loading.gif" lazyload alt="Cross-validation on k" /><figcaption>Cross-validation on k</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Based on the cross-validation results above, choose the best value for k,</span><br><span class="hljs-comment"># retrain the classifier using all the training data, and test it on the test</span><br><span class="hljs-comment"># data. You should be able to get above 28% accuracy on the test data.</span><br>best_k = <span class="hljs-number">10</span><br><br>classifier = KNearestNeighbor()<br>classifier.train(X_train, y_train)<br>y_test_pred = classifier.predict(X_test, k=best_k)<br><br><span class="hljs-comment"># Compute and display the accuracy</span><br>num_correct = np.<span class="hljs-built_in">sum</span>(y_test_pred == y_test)<br>accuracy = <span class="hljs-built_in">float</span>(num_correct) / num_test<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Got %d / %d correct =&gt; accuracy: %f&#x27;</span> % (num_correct, num_test, accuracy))<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># Got 141 / 500 correct =&gt; accuracy: 0.282000</span><br></code></pre></td></tr></table></figure>
<p><strong>Inline Question 3</strong></p>
<p>Which of the following statements about <span class="math inline"><em>k</em></span>-Nearest Neighbor (<span class="math inline"><em>k</em></span>-NN) are true in a classification setting, and for all <span class="math inline"><em>k</em></span>? Select all that apply. 1. The decision boundary of the k-NN classifier is linear. 2. The training error of a 1-NN will always be lower than or equal to that of 5-NN. 3. The test error of a 1-NN will always be lower than that of a 5-NN. 4. The time needed to classify a test example with the k-NN classifier grows with the size of the training set. 5. None of the above.</p>
<p><span class="math inline">$\color{blue}{\textit Your Answer:}$</span> 2, 4</p>
<p><span class="math inline">$\color{blue}{\textit Your Explanation:}$</span> When k=1, each prediction is based on solely point, which may lead to overfitting, when increasing k, the classifier gain the ability of generalization, it perform on training may not as good as 1-NN. For k-NN, all computations are in prediction, so once test a example, it needs to calculate distances to all training points.</p>
<h2 id="q2-implement-a-softmax-classifier">Q2: Implement a Softmax classifier</h2>
<p>这部分将在<strong>softmax.ipynb</strong>中实现一个Softmax图像分类器，会涉及到正则化和优化。</p>
<p>从现在开始，将不会对与之前一样或者相似的步骤进行详细记录。</p>
<h3 id="数据预处理">数据预处理</h3>
<p>运行初始化和导入数据单元格后，对数据进行预处理。首先，划分训练集和验证集，除此之外，还有一个训练数据的小子集用于提高运行效率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Split the data into train, val, and test sets. In addition we will</span><br><span class="hljs-comment"># create a small development set as a subset of the training data;</span><br><span class="hljs-comment"># we can use this for development so our code runs faster.</span><br>num_training = <span class="hljs-number">49000</span><br>num_validation = <span class="hljs-number">1000</span><br>num_test = <span class="hljs-number">1000</span><br>num_dev = <span class="hljs-number">500</span><br><br><span class="hljs-comment"># Our validation set will be num_validation points from the original</span><br><span class="hljs-comment"># training set.</span><br>mask = <span class="hljs-built_in">range</span>(num_training, num_training + num_validation)<br>X_val = X_train[mask]<br>y_val = y_train[mask]<br><br><span class="hljs-comment"># Our training set will be the first num_train points from the original</span><br><span class="hljs-comment"># training set.</span><br>mask = <span class="hljs-built_in">range</span>(num_training)<br>X_train = X_train[mask]<br>y_train = y_train[mask]<br><br><span class="hljs-comment"># We will also make a development set, which is a small subset of</span><br><span class="hljs-comment"># the training set.</span><br>mask = np.random.choice(num_training, num_dev, replace=<span class="hljs-literal">False</span>)<br>X_dev = X_train[mask]<br>y_dev = y_train[mask]<br><br><span class="hljs-comment"># We use the first num_test points of the original test set as our</span><br><span class="hljs-comment"># test set.</span><br>mask = <span class="hljs-built_in">range</span>(num_test)<br>X_test = X_test[mask]<br>y_test = y_test[mask]<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Train data shape: &#x27;</span>, X_train.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Train labels shape: &#x27;</span>, y_train.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Validation data shape: &#x27;</span>, X_val.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Validation labels shape: &#x27;</span>, y_val.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Test data shape: &#x27;</span>, X_test.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Test labels shape: &#x27;</span>, y_test.shape)<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># Train data shape:  (49000, 32, 32, 3)</span><br><span class="hljs-comment"># Train labels shape:  (49000,)</span><br><span class="hljs-comment"># Validation data shape:  (1000, 32, 32, 3)</span><br><span class="hljs-comment"># Validation labels shape:  (1000,)</span><br><span class="hljs-comment"># Test data shape:  (1000, 32, 32, 3)</span><br><span class="hljs-comment"># Test labels shape:  (1000,)</span><br></code></pre></td></tr></table></figure>
<p>接着将图像展平，然后减去均值图像并添加偏项，以优化模型的训练效果和计算效率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Preprocessing: subtract the mean image</span><br><span class="hljs-comment"># first: compute the image mean based on the training data</span><br>mean_image = np.mean(X_train, axis=<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(mean_image[:<span class="hljs-number">10</span>]) <span class="hljs-comment"># print a few of the elements</span><br>plt.figure(figsize=(<span class="hljs-number">4</span>,<span class="hljs-number">4</span>))<br>plt.imshow(mean_image.reshape((<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,<span class="hljs-number">3</span>)).astype(<span class="hljs-string">&#x27;uint8&#x27;</span>)) <span class="hljs-comment"># visualize the mean image</span><br>plt.show()<br><br><span class="hljs-comment"># second: subtract the mean image from train and test data</span><br>X_train -= mean_image<br>X_val -= mean_image<br>X_test -= mean_image<br>X_dev -= mean_image<br><br><span class="hljs-comment"># third: append the bias dimension of ones (i.e. bias trick) so that our classifier</span><br><span class="hljs-comment"># only has to worry about optimizing a single weight matrix W.</span><br>X_train = np.hstack([X_train, np.ones((X_train.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>))])<br>X_val = np.hstack([X_val, np.ones((X_val.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>))])<br>X_test = np.hstack([X_test, np.ones((X_test.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>))])<br>X_dev = np.hstack([X_dev, np.ones((X_dev.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>))])<br><br><span class="hljs-built_in">print</span>(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)<br><br><span class="hljs-comment"># Output1:</span><br><span class="hljs-comment"># [130.64189796 135.98173469 132.47391837 130.05569388 135.34804082</span><br><span class="hljs-comment">#  131.75402041 130.96055102 136.14328571 132.47636735 131.48467347]</span><br><br><span class="hljs-comment"># Output2(见下图)</span><br><br><span class="hljs-comment"># Output3:</span><br><span class="hljs-comment"># (49000, 3073) (1000, 3073) (1000, 3073) (500, 3073)</span><br></code></pre></td></tr></table></figure>
<figure>
<img src="/images/cs231n/mean-image.png" srcset="/imgs/loading.gif" lazyload alt="the image mean based on the training data" /><figcaption>the image mean based on the training data</figcaption>
</figure>
<h3 id="训练和测试">训练和测试</h3>
<p>利用循环计算Softmax损失和梯度，损失函数： <br /><span class="math display">$$L = \frac{1}{N} \sum_{i=1}^N L_i + \lambda \sum_{k} W_k^2$$</span><br /> 第一项为交叉熵损失，第二项为L2正则项，<span class="math inline"><em>L</em><sub><em>i</em></sub></span> 为单个样本 <span class="math inline"><em>x</em><sub><em>i</em></sub></span> （真实标签为 <span class="math inline"><em>y</em><sub><em>i</em></sub></span>）的交叉熵损失，W 为权重矩阵，其中 <span class="math inline"><em>L</em><sub><em>i</em></sub></span> 为： <br /><span class="math display">$$L_i = -log(\frac{e^{s_{y_i}}}{\sum_{j} e^{s_j}}) = -s_{y_i} + log\sum_{j} e^{s_j}$$</span><br /> 得分函数 s 为样本 <span class="math inline"><em>x</em><sub><em>i</em></sub></span> 在各个类别上的得分： <br /><span class="math display"><em>s</em> = <em>x</em><sub><em>i</em></sub><em>W</em></span><br /> 因此，我们可以计算梯度： <br /><span class="math display">$$\nabla_{W}L = \frac{1}{N} \sum_{i=1}^N \nabla_{W}L_i + 2\lambda W$$</span><br /> 其中 <span class="math inline">∇<sub><em>W</em></sub><em>L</em><sub><em>i</em></sub></span> 可以通过链式法则计算： <br /><span class="math display">$$\nabla_{W}L_i = \frac{\partial L_i}{\partial s} \frac{\partial s}{\partial W}$$</span><br /> 但是直接计算 <span class="math inline">$\frac{\partial L_i}{\partial s}$</span> 似乎很难，我们可以改为计算 <span class="math inline">$\frac{\partial L_i}{\partial s_j}$</span>，其中 <span class="math inline"><em>s</em><sub><em>j</em></sub></span> 为样本 <span class="math inline"><em>x</em><sub><em>i</em></sub></span> 在第 j 类上的得分： <br /><span class="math display"><em>s</em><sub><em>j</em></sub> = <em>x</em><sub><em>i</em></sub><em>W</em><sub><em>j</em></sub></span><br /> <br /><span class="math display">$$\nabla_{W_{j}}L_i = \frac{\partial L_i}{\partial s_j} \frac{\partial s_j}{\partial W_j}$$</span><br /></p>
<blockquote>
<p>这里 <span class="math inline"><em>W</em><sub><em>j</em></sub></span> 表示在第 j 类上的权重，所以在后面的代码中是 W 的第 j 列而不是第 j 行。</p>
</blockquote>
<p>从元素组成上来看，很容易发现 <span class="math inline">∇<sub><em>W</em></sub><em>L</em><sub><em>i</em></sub></span> 由 <span class="math inline">∇<sub><em>W</em><sub><em>j</em></sub></sub><em>L</em><sub><em>i</em></sub></span> （j = 1, 2, …, num_classes）组成，这里 <span class="math inline">∇<sub><em>W</em></sub><em>L</em><sub><em>i</em></sub></span> 的第 j 列就是 <span class="math inline">∇<sub><em>W</em><sub><em>j</em></sub></sub><em>L</em><sub><em>i</em></sub></span> 。</p>
<p>根据上面的式子，容易得到： <br /><span class="math display">$$\frac{\partial L_i}{\partial s_j} = \begin{cases} -1 + \frac{e^{s_{j}}}{\sum_{k} e^{s_k}} &amp; \text{if } j = y_i, \\ \frac{e^{s_{j}}}{\sum_{k} e^{s_k}} &amp; \text{if } j \neq y_i. \end{cases}$$</span><br /> <br /><span class="math display">$$\frac{\partial s_j}{\partial W_j} = x_i^T$$</span><br /> 现在，解析梯度的计算就可以开始了，在代码中，会用 <span class="math inline">$p_j = \frac{e^{s_{j}}}{\sum_{k} e^{s_k}}$</span> 来稍微简化上面的式子，实现<code>cs231n/classifiers/softmax.py</code>中的<code>softmax_loss_naive</code>函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># In cs231n/classifiers/softmax.py</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax_loss_naive</span>(<span class="hljs-params">W, X, y, reg</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Softmax loss function, naive implementation (with loops)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs have dimension D, there are C classes, and we operate on minibatches</span><br><span class="hljs-string">    of N examples.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - W: A numpy array of shape (D, C) containing weights.</span><br><span class="hljs-string">    - X: A numpy array of shape (N, D) containing a minibatch of data.</span><br><span class="hljs-string">    - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span><br><span class="hljs-string">      that X[i] has label c, where 0 &lt;= c &lt; C.</span><br><span class="hljs-string">    - reg: (float) regularization strength</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns a tuple of:</span><br><span class="hljs-string">    - loss as single float</span><br><span class="hljs-string">    - gradient with respect to weights W; an array of same shape as W</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># Initialize the loss and gradient to zero.</span><br>    loss = <span class="hljs-number">0.0</span><br>    dW = np.zeros_like(W)<br><br>    <span class="hljs-comment"># compute the loss and the gradient</span><br>    num_classes = W.shape[<span class="hljs-number">1</span>]<br>    num_train = X.shape[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_train):<br>        scores = X[i].dot(W)<br><br>        <span class="hljs-comment"># compute the probabilities in numerically stable way</span><br>        scores -= np.<span class="hljs-built_in">max</span>(scores)<br>        p = np.exp(scores)<br>        p /= p.<span class="hljs-built_in">sum</span>()  <span class="hljs-comment"># normalize</span><br>        logp = np.log(p)<br><br>        loss -= logp[y[i]]  <span class="hljs-comment"># negative log probability is the loss</span><br>        <br>        <span class="hljs-comment"># compute the gradient</span><br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_classes):<br>            dW[:, j] += (p[j] - (j == y[i])) * X[i]<br><br>    <span class="hljs-comment"># normalized hinge loss plus regularization</span><br>    loss = loss / num_train + reg * np.<span class="hljs-built_in">sum</span>(W * W)<br><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                     #</span><br>    <span class="hljs-comment"># Compute the gradient of the loss function and store it dW.                #</span><br>    <span class="hljs-comment"># Rather that first computing the loss and then computing the derivative,   #</span><br>    <span class="hljs-comment"># it may be simpler to compute the derivative at the same time that the     #</span><br>    <span class="hljs-comment"># loss is being computed. As a result you may need to modify some of the    #</span><br>    <span class="hljs-comment"># code above to compute the gradient.                                       #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    dW = dW / num_train + <span class="hljs-number">2</span> * reg * W<br><br>    <span class="hljs-keyword">return</span> loss, dW<br></code></pre></td></tr></table></figure>
<p>测试损失和梯度的计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Evaluate the naive implementation of the loss we provided for you:</span><br><span class="hljs-keyword">from</span> cs231n.classifiers.softmax <span class="hljs-keyword">import</span> softmax_loss_naive<br><span class="hljs-keyword">import</span> time<br><br><span class="hljs-comment"># generate a random Softmax classifier weight matrix of small numbers</span><br>W = np.random.randn(<span class="hljs-number">3073</span>, <span class="hljs-number">10</span>) * <span class="hljs-number">0.0001</span><br><br>loss, grad = softmax_loss_naive(W, X_dev, y_dev, <span class="hljs-number">0.000005</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;loss: %f&#x27;</span> % (loss, ))<br><br><span class="hljs-comment"># As a rough sanity check, our loss should be something close to -log(0.1).</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;loss: %f&#x27;</span> % loss)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;sanity check: %f&#x27;</span> % (-np.log(<span class="hljs-number">0.1</span>)))<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># loss: 2.344003</span><br><span class="hljs-comment"># loss: 2.344003</span><br><span class="hljs-comment"># sanity check: 2.302585</span><br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Once you&#x27;ve implemented the gradient, recompute it with the code below</span><br><span class="hljs-comment"># and gradient check it with the function we provided for you</span><br><br><span class="hljs-comment"># Compute the loss and its gradient at W.</span><br>loss, grad = softmax_loss_naive(W, X_dev, y_dev, <span class="hljs-number">0.0</span>)<br><br><span class="hljs-comment"># Numerically compute the gradient along several randomly chosen dimensions, and</span><br><span class="hljs-comment"># compare them with your analytically computed gradient. The numbers should match</span><br><span class="hljs-comment"># almost exactly along all dimensions.</span><br><span class="hljs-keyword">from</span> cs231n.gradient_check <span class="hljs-keyword">import</span> grad_check_sparse<br>f = <span class="hljs-keyword">lambda</span> w: softmax_loss_naive(w, X_dev, y_dev, <span class="hljs-number">0.0</span>)[<span class="hljs-number">0</span>]<br>grad_numerical = grad_check_sparse(f, W, grad)<br><br><span class="hljs-comment"># do the gradient check once again with regularization turned on</span><br><span class="hljs-comment"># you didn&#x27;t forget the regularization gradient did you?</span><br>loss, grad = softmax_loss_naive(W, X_dev, y_dev, <span class="hljs-number">5e1</span>)<br>f = <span class="hljs-keyword">lambda</span> w: softmax_loss_naive(w, X_dev, y_dev, <span class="hljs-number">5e1</span>)[<span class="hljs-number">0</span>]<br>grad_numerical = grad_check_sparse(f, W, grad)<br><br><span class="hljs-comment"># Output</span><br><span class="hljs-comment"># numerical: 0.637966 analytic: 0.637965, relative error: 1.236012e-07</span><br><span class="hljs-comment"># numerical: -0.354512 analytic: -0.354512, relative error: 1.075945e-08</span><br><span class="hljs-comment"># numerical: 0.111390 analytic: 0.111390, relative error: 1.455617e-07</span><br><span class="hljs-comment"># numerical: -1.616721 analytic: -1.616721, relative error: 8.005533e-08</span><br><span class="hljs-comment"># numerical: 1.509032 analytic: 1.509032, relative error: 2.254300e-08</span><br><span class="hljs-comment"># numerical: 1.250068 analytic: 1.250068, relative error: 7.258522e-08</span><br><span class="hljs-comment"># numerical: 3.575710 analytic: 3.575710, relative error: 2.251164e-08</span><br><span class="hljs-comment"># numerical: -2.220965 analytic: -2.220966, relative error: 1.706946e-08</span><br><span class="hljs-comment"># numerical: -0.745233 analytic: -0.745233, relative error: 4.451017e-08</span><br><span class="hljs-comment"># numerical: -4.452108 analytic: -4.452107, relative error: 6.863113e-09</span><br><span class="hljs-comment"># numerical: -0.437320 analytic: -0.437320, relative error: 4.286594e-08</span><br><span class="hljs-comment"># numerical: 2.110840 analytic: 2.110840, relative error: 2.802770e-09</span><br><span class="hljs-comment"># numerical: 1.268882 analytic: 1.268882, relative error: 9.150069e-08</span><br><span class="hljs-comment"># numerical: -2.231693 analytic: -2.231693, relative error: 2.893781e-08</span><br><span class="hljs-comment"># numerical: 1.904050 analytic: 1.904049, relative error: 2.485119e-08</span><br><span class="hljs-comment"># numerical: 1.144526 analytic: 1.144526, relative error: 1.254521e-08</span><br><span class="hljs-comment"># numerical: 1.573668 analytic: 1.573668, relative error: 5.874951e-08</span><br><span class="hljs-comment"># numerical: 0.279931 analytic: 0.279931, relative error: 3.284202e-07</span><br><span class="hljs-comment"># numerical: 2.321664 analytic: 2.321663, relative error: 9.635676e-09</span><br><span class="hljs-comment"># numerical: 4.186737 analytic: 4.186737, relative error: 1.598212e-10</span><br></code></pre></td></tr></table></figure>
<p><strong>Inline Question 1</strong></p>
<p>Why do we expect our loss to be close to -log(0.1)? Explain briefly.</p>
<p><span class="math inline">$\color{blue}{\textit Your Answer:}$</span> <em>When the weights W are initialized to small random values, the logits (scores) computed as X </em> W will be close to zero for all classes. Applying the softmax function to these small logits results in a probability distribution over classes that is approximately uniform. For a 10-class classification task, each class thus has a probability close to 0.1.*</p>
<p><strong>Inline Question 2</strong></p>
<p>Although gradcheck is reliable softmax loss, it is possible that for SVM loss, once in a while, a dimension in the gradcheck will not match exactly. What could such a discrepancy be caused by? Is it a reason for concern? What is a simple example in one dimension where a svm loss gradient check could fail? How would change the margin affect of the frequency of this happening?</p>
<p>Note that SVM loss for a sample <span class="math inline">(<em>x</em><sub><em>i</em></sub>, <em>y</em><sub><em>i</em></sub>)</span> is defined as: <br /><span class="math display"><em>L</em><sub><em>i</em></sub> = ∑<sub><em>j</em> ≠ <em>y</em><sub><em>i</em></sub></sub>max (0, <em>s</em><sub><em>j</em></sub> − <em>s</em><sub><em>y</em><sub><em>i</em></sub></sub> + <em>Δ</em>)</span><br /> where <span class="math inline"><em>j</em></span> iterates over all classes except the correct class <span class="math inline"><em>y</em><sub><em>i</em></sub></span> and <span class="math inline"><em>s</em><sub><em>j</em></sub></span> denotes the classifier score for <span class="math inline"><em>j</em><sup><em>t</em><em>h</em></sup></span> class. <span class="math inline"><em>Δ</em></span> is a scalar margin. For more information, refer to ‘Multiclass Support Vector Machine loss’ on <a target="_blank" rel="noopener" href="https://cs231n.github.io/linear-classify/">this</a> page.</p>
<p><em>Hint: the SVM loss function is not strictly speaking differentiable.</em></p>
<p><span class="math inline">$\color{blue}{\textit Your Answer:}$</span> <em>For SVM loss, in gradcheck, a dimension may fail to match because the numerical precision or the loss function’s not differentiable. This is not an implementation error but an inherent property of the loss function itself. Example, with a single feature, the gradient at a point is discontinuous, leading to potential inconsistencies between the numerical and analytical gradients. Increasing the margin can reduces the probability of kinks occurring, minimizing mismatches.</em></p>
<p>向量化计算Softmax损失和梯度，在之前的基础上，类似地，<span class="math inline">$\frac{\partial L_i}{\partial s}$</span> 由 <span class="math inline">$\frac{\partial L_i}{\partial s_j}$</span> 组成，则计算出 <span class="math inline">$\frac{\partial L_i}{\partial s}$</span> 后，可以使用便于向量化计算的公式： <br /><span class="math display">$$\nabla_{W}L_i = \frac{\partial L_i}{\partial s} \frac{\partial s}{\partial W}$$</span><br /> 现在，实现<code>cs231n/classifiers/softmax.py</code>中的<code>softmax_loss_vectorized</code>函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># In cs231n/classifiers/softmax.py</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax_loss_vectorized</span>(<span class="hljs-params">W, X, y, reg</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Softmax loss function, vectorized version.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs and outputs are the same as softmax_loss_naive.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># Initialize the loss and gradient to zero.</span><br>    loss = <span class="hljs-number">0.0</span><br>    dW = np.zeros_like(W)<br><br><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                     #</span><br>    <span class="hljs-comment"># Implement a vectorized version of the softmax loss, storing the           #</span><br>    <span class="hljs-comment"># result in loss.                                                           #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    num_train = X.shape[<span class="hljs-number">0</span>]<br>    scores = X.dot(W)<br>    scores -= np.reshape(np.<span class="hljs-built_in">max</span>(scores, axis=<span class="hljs-number">1</span>), (num_train, <span class="hljs-number">1</span>))<br>    p = np.exp(scores)<br>    p /= np.reshape(np.<span class="hljs-built_in">sum</span>(p, axis=<span class="hljs-number">1</span>), (num_train, <span class="hljs-number">1</span>))<br>    logp = np.log(p)<br>    loss -= np.<span class="hljs-built_in">sum</span>(logp[np.arange(num_train), y])<br>    loss = loss / num_train + reg * np.<span class="hljs-built_in">sum</span>(W * W)<br><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                     #</span><br>    <span class="hljs-comment"># Implement a vectorized version of the gradient for the softmax            #</span><br>    <span class="hljs-comment"># loss, storing the result in dW.                                           #</span><br>    <span class="hljs-comment">#                                                                           #</span><br>    <span class="hljs-comment"># Hint: Instead of computing the gradient from scratch, it may be easier    #</span><br>    <span class="hljs-comment"># to reuse some of the intermediate values that you used to compute the     #</span><br>    <span class="hljs-comment"># loss.                                                                     #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    dscores = p<br>    dscores[np.arange(num_train), y] -= <span class="hljs-number">1</span><br>    dW = X.T.dot(dscores)<br>    dW = dW / num_train + <span class="hljs-number">2</span> * reg * W<br><br>    <span class="hljs-keyword">return</span> loss, dW<br></code></pre></td></tr></table></figure>
<p>测试损失和梯度的计算，并于之前的循环方法对比运行时间：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Next implement the function softmax_loss_vectorized; for now only compute the loss;</span><br><span class="hljs-comment"># we will implement the gradient in a moment.</span><br>tic = time.time()<br>loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, <span class="hljs-number">0.000005</span>)<br>toc = time.time()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Naive loss: %e computed in %fs&#x27;</span> % (loss_naive, toc - tic))<br><br><span class="hljs-keyword">from</span> cs231n.classifiers.softmax <span class="hljs-keyword">import</span> softmax_loss_vectorized<br>tic = time.time()<br>loss_vectorized, _ = softmax_loss_vectorized(W, X_dev, y_dev, <span class="hljs-number">0.000005</span>)<br>toc = time.time()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Vectorized loss: %e computed in %fs&#x27;</span> % (loss_vectorized, toc - tic))<br><br><span class="hljs-comment"># The losses should match but your vectorized implementation should be much faster.</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;difference: %f&#x27;</span> % (loss_naive - loss_vectorized))<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># Naive loss: 2.344003e+00 computed in 0.058616s</span><br><span class="hljs-comment"># Vectorized loss: 2.344003e+00 computed in 0.010929s</span><br><span class="hljs-comment"># difference: -0.000000</span><br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Complete the implementation of softmax_loss_vectorized, and compute the gradient</span><br><span class="hljs-comment"># of the loss function in a vectorized way.</span><br><br><span class="hljs-comment"># The naive implementation and the vectorized implementation should match, but</span><br><span class="hljs-comment"># the vectorized version should still be much faster.</span><br>tic = time.time()<br>_, grad_naive = softmax_loss_naive(W, X_dev, y_dev, <span class="hljs-number">0.000005</span>)<br>toc = time.time()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Naive loss and gradient: computed in %fs&#x27;</span> % (toc - tic))<br><br>tic = time.time()<br>_, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, <span class="hljs-number">0.000005</span>)<br>toc = time.time()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Vectorized loss and gradient: computed in %fs&#x27;</span> % (toc - tic))<br><br><span class="hljs-comment"># The loss is a single number, so it is easy to compare the values computed</span><br><span class="hljs-comment"># by the two implementations. The gradient on the other hand is a matrix, so</span><br><span class="hljs-comment"># we use the Frobenius norm to compare them.</span><br>difference = np.linalg.norm(grad_naive - grad_vectorized, <span class="hljs-built_in">ord</span>=<span class="hljs-string">&#x27;fro&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;difference: %f&#x27;</span> % difference)<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># Naive loss and gradient: computed in 0.100571s</span><br><span class="hljs-comment"># Vectorized loss and gradient: computed in 0.019927s</span><br><span class="hljs-comment"># difference: 0.000000</span><br></code></pre></td></tr></table></figure>
<h3 id="stochastic-gradient-descent">Stochastic Gradient Descent</h3>
<p>使用SGD来减小损失，实现<code>cs231n/classifiers/linear_classifier.py</code>中的<code>train</code>方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># In cs231n/classifiers/linear_classifier.py</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        X,</span><br><span class="hljs-params">        y,</span><br><span class="hljs-params">        learning_rate=<span class="hljs-number">1e-3</span>,</span><br><span class="hljs-params">        reg=<span class="hljs-number">1e-5</span>,</span><br><span class="hljs-params">        num_iters=<span class="hljs-number">100</span>,</span><br><span class="hljs-params">        batch_size=<span class="hljs-number">200</span>,</span><br><span class="hljs-params">        verbose=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Train this linear classifier using stochastic gradient descent.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - X: A numpy array of shape (N, D) containing training data; there are N</span><br><span class="hljs-string">          training samples each of dimension D.</span><br><span class="hljs-string">        - y: A numpy array of shape (N,) containing training labels; y[i] = c</span><br><span class="hljs-string">          means that X[i] has label 0 &lt;= c &lt; C for C classes.</span><br><span class="hljs-string">        - learning_rate: (float) learning rate for optimization.</span><br><span class="hljs-string">        - reg: (float) regularization strength.</span><br><span class="hljs-string">        - num_iters: (integer) number of steps to take when optimizing</span><br><span class="hljs-string">        - batch_size: (integer) number of training examples to use at each step.</span><br><span class="hljs-string">        - verbose: (boolean) If true, print progress during optimization.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Outputs:</span><br><span class="hljs-string">        A list containing the value of the loss function at each training iteration.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        num_train, dim = X.shape<br>        num_classes = (<br>            np.<span class="hljs-built_in">max</span>(y) + <span class="hljs-number">1</span><br>        )  <span class="hljs-comment"># assume y takes values 0...K-1 where K is number of classes</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.W <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># lazily initialize W</span><br>            <span class="hljs-variable language_">self</span>.W = <span class="hljs-number">0.001</span> * np.random.randn(dim, num_classes)<br><br>        <span class="hljs-comment"># Run stochastic gradient descent to optimize W</span><br>        loss_history = []<br>        <span class="hljs-keyword">for</span> it <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iters):<br>            X_batch = <span class="hljs-literal">None</span><br>            y_batch = <span class="hljs-literal">None</span><br><br>            <span class="hljs-comment">#########################################################################</span><br>            <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                 #</span><br>            <span class="hljs-comment"># Sample batch_size elements from the training data and their           #</span><br>            <span class="hljs-comment"># corresponding labels to use in this round of gradient descent.        #</span><br>            <span class="hljs-comment"># Store the data in X_batch and their corresponding labels in           #</span><br>            <span class="hljs-comment"># y_batch; after sampling X_batch should have shape (batch_size, dim)   #</span><br>            <span class="hljs-comment"># and y_batch should have shape (batch_size,)                           #</span><br>            <span class="hljs-comment">#                                                                       #</span><br>            <span class="hljs-comment"># Hint: Use np.random.choice to generate indices. Sampling with         #</span><br>            <span class="hljs-comment"># replacement is faster than sampling without replacement.              #</span><br>            <span class="hljs-comment">#########################################################################</span><br>            indices = np.random.choice(num_train, size=batch_size, replace=<span class="hljs-literal">True</span>)<br>            X_batch = X[indices]<br>            y_batch = y[indices]<br><br>            <span class="hljs-comment"># evaluate loss and gradient</span><br>            loss, grad = <span class="hljs-variable language_">self</span>.loss(X_batch, y_batch, reg)<br>            loss_history.append(loss)<br><br>            <span class="hljs-comment"># perform parameter update</span><br>            <span class="hljs-comment">#########################################################################</span><br>            <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                 #</span><br>            <span class="hljs-comment"># Update the weights using the gradient and the learning rate.          #</span><br>            <span class="hljs-comment">#########################################################################</span><br>            <span class="hljs-variable language_">self</span>.W -= learning_rate * grad<br><br>            <span class="hljs-keyword">if</span> verbose <span class="hljs-keyword">and</span> it % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;iteration %d / %d: loss %f&quot;</span> % (it, num_iters, loss))<br><br>        <span class="hljs-keyword">return</span> loss_history<br></code></pre></td></tr></table></figure>
<p>训练模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># In the file linear_classifier.py, implement SGD in the function</span><br><span class="hljs-comment"># LinearClassifier.train() and then run it with the code below.</span><br><span class="hljs-keyword">from</span> cs231n.classifiers <span class="hljs-keyword">import</span> Softmax<br>softmax = Softmax()<br>tic = time.time()<br>loss_hist = softmax.train(X_train, y_train, learning_rate=<span class="hljs-number">1e-7</span>, reg=<span class="hljs-number">2.5e4</span>,<br>                      num_iters=<span class="hljs-number">1500</span>, verbose=<span class="hljs-literal">True</span>)<br>toc = time.time()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;That took %fs&#x27;</span> % (toc - tic))<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># iteration 0 / 1500: loss 784.261531</span><br><span class="hljs-comment"># iteration 100 / 1500: loss 287.556184</span><br><span class="hljs-comment"># iteration 200 / 1500: loss 106.569326</span><br><span class="hljs-comment"># iteration 300 / 1500: loss 40.286102</span><br><span class="hljs-comment"># iteration 400 / 1500: loss 15.996293</span><br><span class="hljs-comment"># iteration 500 / 1500: loss 7.167806</span><br><span class="hljs-comment"># iteration 600 / 1500: loss 3.924627</span><br><span class="hljs-comment"># iteration 700 / 1500: loss 2.739457</span><br><span class="hljs-comment"># iteration 800 / 1500: loss 2.365508</span><br><span class="hljs-comment"># iteration 900 / 1500: loss 2.133507</span><br><span class="hljs-comment"># iteration 1000 / 1500: loss 2.143132</span><br><span class="hljs-comment"># iteration 1100 / 1500: loss 2.142237</span><br><span class="hljs-comment"># iteration 1200 / 1500: loss 2.054092</span><br><span class="hljs-comment"># iteration 1300 / 1500: loss 2.097079</span><br><span class="hljs-comment"># iteration 1400 / 1500: loss 2.074805</span><br><span class="hljs-comment"># That took 11.690820s</span><br></code></pre></td></tr></table></figure>
<p>可视化结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># A useful debugging strategy is to plot the loss as a function of</span><br><span class="hljs-comment"># iteration number:</span><br>plt.plot(loss_hist)<br>plt.xlabel(<span class="hljs-string">&#x27;Iteration number&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss value&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>
<figure>
<img src="/images/cs231n/loss-history.png" srcset="/imgs/loading.gif" lazyload alt="the loss as a function of iteration number" /><figcaption>the loss as a function of iteration number</figcaption>
</figure>
<p>实现<code>cs231n/classifiers/linear_classifier.py</code>中的<code>predict</code>方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># In cs231n/classifiers/linear_classifier.py</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Use the trained weights of this linear classifier to predict labels for</span><br><span class="hljs-string">        data points.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - X: A numpy array of shape (N, D) containing training data; there are N</span><br><span class="hljs-string">          training samples each of dimension D.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional</span><br><span class="hljs-string">          array of length N, and each element is an integer giving the predicted</span><br><span class="hljs-string">          class.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        y_pred = np.zeros(X.shape[<span class="hljs-number">0</span>])<br>        <span class="hljs-comment">###########################################################################</span><br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                   #</span><br>        <span class="hljs-comment"># Implement this method. Store the predicted labels in y_pred.            #</span><br>        <span class="hljs-comment">###########################################################################</span><br>        scores = X.dot(<span class="hljs-variable language_">self</span>.W)<br>        y_pred = np.argmax(scores,axis=<span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">return</span> y_pred<br></code></pre></td></tr></table></figure>
<p>使用模型预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Write the LinearClassifier.predict function and evaluate the performance on</span><br><span class="hljs-comment"># both the training and validation set</span><br><span class="hljs-comment"># You should get validation accuracy of about 0.34 (&gt; 0.33).</span><br>y_train_pred = softmax.predict(X_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;training accuracy: %f&#x27;</span> % (np.mean(y_train == y_train_pred), ))<br>y_val_pred = softmax.predict(X_val)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;validation accuracy: %f&#x27;</span> % (np.mean(y_val == y_val_pred), ))<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># training accuracy: 0.328122</span><br><span class="hljs-comment"># validation accuracy: 0.343000</span><br></code></pre></td></tr></table></figure>
<p>保存模型（具体代码在cs231n/classifiers/linear_classifier.py中，这里实际上是保存W矩阵）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Save the trained model for autograder.</span><br>softmax.save(<span class="hljs-string">&quot;softmax.npy&quot;</span>)<br></code></pre></td></tr></table></figure>
<p>交叉验证调整超参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Use the validation set to tune hyperparameters (regularization strength and</span><br><span class="hljs-comment"># learning rate). You should experiment with different ranges for the learning</span><br><span class="hljs-comment"># rates and regularization strengths; if you are careful you should be able to</span><br><span class="hljs-comment"># get a classification accuracy of about 0.365 (&gt; 0.36) on the validation set.</span><br><br><span class="hljs-comment"># Note: you may see runtime/overflow warnings during hyper-parameter search.</span><br><span class="hljs-comment"># This may be caused by extreme values, and is not a bug.</span><br><br><span class="hljs-comment"># results is dictionary mapping tuples of the form</span><br><span class="hljs-comment"># (learning_rate, regularization_strength) to tuples of the form</span><br><span class="hljs-comment"># (training_accuracy, validation_accuracy). The accuracy is simply the fraction</span><br><span class="hljs-comment"># of data points that are correctly classified.</span><br>results = &#123;&#125;<br>best_val = -<span class="hljs-number">1</span>   <span class="hljs-comment"># The highest validation accuracy that we have seen so far.</span><br>best_softmax = <span class="hljs-literal">None</span> <span class="hljs-comment"># The Softmax object that achieved the highest validation rate.</span><br><br><span class="hljs-comment">################################################################################</span><br><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                        #</span><br><span class="hljs-comment"># Write code that chooses the best hyperparameters by tuning on the validation #</span><br><span class="hljs-comment"># set. For each combination of hyperparameters, train a Softmax on the.        #</span><br><span class="hljs-comment"># training set, compute its accuracy on the training and validation sets, and  #</span><br><span class="hljs-comment"># store these numbers in the results dictionary. In addition, store the best   #</span><br><span class="hljs-comment"># validation accuracy in best_val and the Softmax object that achieves this.   #</span><br><span class="hljs-comment"># accuracy in best_softmax.                                                    #</span><br><span class="hljs-comment">#                                                                              #</span><br><span class="hljs-comment"># Hint: You should use a small value for num_iters as you develop your         #</span><br><span class="hljs-comment"># validation code so that the classifiers don&#x27;t take much time to train; once  #</span><br><span class="hljs-comment"># you are confident that your validation code works, you should rerun the      #</span><br><span class="hljs-comment"># code with a larger value for num_iters.                                      #</span><br><span class="hljs-comment">################################################################################</span><br><br><span class="hljs-comment"># Provided as a reference. You may or may not want to change these hyperparameters</span><br>learning_rates = [<span class="hljs-number">1e-7</span>, <span class="hljs-number">1e-6</span>]<br>regularization_strengths = [<span class="hljs-number">2.5e4</span>, <span class="hljs-number">1e4</span>]<br><br><span class="hljs-keyword">for</span> lr <span class="hljs-keyword">in</span> learning_rates:<br>  <span class="hljs-keyword">for</span> reg <span class="hljs-keyword">in</span> regularization_strengths:<br>    soft = Softmax()<br>    softmax.train(X_train, y_train, learning_rate=lr, reg=reg, num_iters=<span class="hljs-number">1500</span>)<br>    y_train_pred = softmax.predict(X_train)<br>    y_val_pred = softmax.predict(X_val)<br>    training_accuracy = np.mean(y_train == y_train_pred)<br>    validation_accuracy = np.mean(y_val == y_val_pred)<br>    results[(lr, reg)] = (training_accuracy, validation_accuracy)<br>    <span class="hljs-keyword">if</span> validation_accuracy &gt; best_val:<br>      best_val = validation_accuracy<br>      best_softmax = softmax<br><br><span class="hljs-comment"># Print out results.</span><br><span class="hljs-keyword">for</span> lr, reg <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(results):<br>    train_accuracy, val_accuracy = results[(lr, reg)]<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;lr %e reg %e train accuracy: %f val accuracy: %f&#x27;</span> % (<br>                lr, reg, train_accuracy, val_accuracy))<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;best validation accuracy achieved during cross-validation: %f&#x27;</span> % best_val)<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.356714 val accuracy: 0.371000</span><br><span class="hljs-comment"># lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.326122 val accuracy: 0.342000</span><br><span class="hljs-comment"># lr 1.000000e-06 reg 1.000000e+04 train accuracy: 0.350102 val accuracy: 0.355000</span><br><span class="hljs-comment"># lr 1.000000e-06 reg 2.500000e+04 train accuracy: 0.321857 val accuracy: 0.334000</span><br><span class="hljs-comment"># best validation accuracy achieved during cross-validation: 0.371000</span><br></code></pre></td></tr></table></figure>
<p>结果可视化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Visualize the cross-validation results</span><br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> pdb<br><br><span class="hljs-comment"># pdb.set_trace()</span><br><br>x_scatter = [math.log10(x[<span class="hljs-number">0</span>]) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> results]<br>y_scatter = [math.log10(x[<span class="hljs-number">1</span>]) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> results]<br><br><span class="hljs-comment"># plot training accuracy</span><br>marker_size = <span class="hljs-number">100</span><br>colors = [results[x][<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> results]<br>plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>plt.tight_layout(pad=<span class="hljs-number">3</span>)<br>plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap=plt.cm.coolwarm)<br>plt.colorbar()<br>plt.xlabel(<span class="hljs-string">&#x27;log learning rate&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;log regularization strength&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;CIFAR-10 training accuracy&#x27;</span>)<br><br><span class="hljs-comment"># plot validation accuracy</span><br>colors = [results[x][<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> results] <span class="hljs-comment"># default size of markers is 20</span><br>plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap=plt.cm.coolwarm)<br>plt.colorbar()<br>plt.xlabel(<span class="hljs-string">&#x27;log learning rate&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;log regularization strength&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;CIFAR-10 validation accuracy&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>
<p>使用得到的表现最好的模型进行测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Evaluate the best softmax on test set</span><br>y_test_pred = best_softmax.predict(X_test)<br>test_accuracy = np.mean(y_test == y_test_pred)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Softmax classifier on raw pixels final test set accuracy: %f&#x27;</span> % test_accuracy)<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># Softmax classifier on raw pixels final test set accuracy: 0.359000</span><br></code></pre></td></tr></table></figure>
<p>保存模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Save best softmax model</span><br>best_softmax.save(<span class="hljs-string">&quot;best_softmax.npy&quot;</span>)<br></code></pre></td></tr></table></figure>
<p>可视化模型学习到的各个类别的权重：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Visualize the learned weights for each class.</span><br><span class="hljs-comment"># Depending on your choice of learning rate and regularization strength, these may</span><br><span class="hljs-comment"># or may not be nice to look at.</span><br>w = best_softmax.W[:-<span class="hljs-number">1</span>,:] <span class="hljs-comment"># strip out the bias</span><br>w = w.reshape(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>)<br>w_min, w_max = np.<span class="hljs-built_in">min</span>(w), np.<span class="hljs-built_in">max</span>(w)<br>classes = [<span class="hljs-string">&#x27;plane&#x27;</span>, <span class="hljs-string">&#x27;car&#x27;</span>, <span class="hljs-string">&#x27;bird&#x27;</span>, <span class="hljs-string">&#x27;cat&#x27;</span>, <span class="hljs-string">&#x27;deer&#x27;</span>, <span class="hljs-string">&#x27;dog&#x27;</span>, <span class="hljs-string">&#x27;frog&#x27;</span>, <span class="hljs-string">&#x27;horse&#x27;</span>, <span class="hljs-string">&#x27;ship&#x27;</span>, <span class="hljs-string">&#x27;truck&#x27;</span>]<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, i + <span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># Rescale the weights to be between 0 and 255</span><br>    wimg = <span class="hljs-number">255.0</span> * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)<br>    plt.imshow(wimg.astype(<span class="hljs-string">&#x27;uint8&#x27;</span>))<br>    plt.axis(<span class="hljs-string">&#x27;off&#x27;</span>)<br>    plt.title(classes[i])<br></code></pre></td></tr></table></figure>
<figure>
<img src="/images/cs231n/softmax-weights.png" srcset="/imgs/loading.gif" lazyload alt="Visualize the learned weights for each class" /><figcaption>Visualize the learned weights for each class</figcaption>
</figure>
<p><strong>Inline question 3</strong></p>
<p>Describe what your visualized Softmax classifier weights look like, and offer a brief explanation for why they look the way they do.</p>
<p><span class="math inline">$\color{blue}{\textit Your Answer:}$</span> <em>The visualized Softmax classifier weights typically show blurred, low-contrast “templates” that roughly correspond to the key visual features of each class. For example, the weights of class car look like a body of car and its windows. Because the Softmax classifier learns linear decision boundaries. The weights act as feature detectors: during training, each class’s weights adjust to amplify pixel values that are statistically characteristic of that class and suppress irrelevant ones. The blurriness results from the model averaging over diverse training examples, capturing commonalities rather than sharp details. Regularization also smooths the weights, preventing overfitting to noise in the data.</em></p>
<p><strong>Inline Question 4</strong> - <em>True or False</em></p>
<p>Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would change the softmax loss, but leave the SVM loss unchanged.</p>
<p><span class="math inline">$\color{blue}{\textit Your Answer:}$</span> True</p>
<p><span class="math inline">$\color{blue}{\textit Your Explanation:}$</span> For the SVM loss, a new datapoint leaves the loss unchanged if, for its true class <span class="math inline"><em>y</em><sub><em>i</em></sub></span>, all other classes <span class="math inline"><em>j</em> ≠ <em>y</em><sub><em>i</em></sub></span> satisfy <span class="math inline"><em>s</em><sub><em>j</em></sub> − <em>s</em><sub><em>y</em><sub><em>i</em></sub></sub> + <em>Δ</em> ≤ 0</span> (i.e., no margins are violated). In this case, the max(0, ·) term for all <span class="math inline"><em>j</em> ≠ <em>y</em><sub><em>i</em></sub></span> is 0, so the per-datapoint SVM loss is 0, and adding it does not change the total loss. For the softmax loss, however, the loss for a datapoint depends on the probability of its true class (<span class="math inline"> − <em>l</em><em>o</em><em>g</em>(<em>p</em><sub><em>y</em><sub><em>i</em></sub></sub>)</span>), which is never exactly 0 (since softmax probabilities are always positive).</p>
<h2 id="q3-two-layer-neural-network">Q3: Two-Layer Neural Network</h2>
<p>这部分主要关注<code>two_layer_net.ipynb</code>，将实现一个两层的神经网络。</p>
<blockquote>
<p>从这里开始，将减少一些不是很重要的细节，相信能看到这里，一些细节不需要详细的描述了。</p>
</blockquote>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" class="category-chain-item">计算机</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" class="print-no-link">#计算机视觉</a>
      
        <a href="/tags/AI/" class="print-no-link">#AI</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>CS231n：Assignment1</div>
      <div>https://logicff.github.io/2025/11/14/cs231n-1/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>logicff</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年11月14日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="NC - 非商业性使用">
                    <i class="iconfont icon-cc-nc"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="SA - 相同方式共享">
                    <i class="iconfont icon-cc-sa"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/11/09/6.5840-2/" title="6.5840：Lab2 - Key/Value Server">
                        <span class="hidden-mobile">6.5840：Lab2 - Key/Value Server</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
