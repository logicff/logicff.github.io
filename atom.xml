<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>logicff</title>
  
  
  <link href="https://logicff.github.io/atom.xml" rel="self"/>
  
  <link href="https://logicff.github.io/"/>
  <updated>2025-11-18T13:56:33.000Z</updated>
  <id>https://logicff.github.io/</id>
  
  <author>
    <name>logicff</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CS231n：Assignment1</title>
    <link href="https://logicff.github.io/2025/11/14/cs231n-1/"/>
    <id>https://logicff.github.io/2025/11/14/cs231n-1/</id>
    <published>2025-11-14T12:33:27.000Z</published>
    <updated>2025-11-18T13:56:33.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>由于毕设需要，且没怎么学习过AI这块，所以浅浅记录一下，但不会像之前记录MIT6.5840那样在实现细节上一笔带过。原因的话，对于分布式这块，好歹我有过一点分布式系统的经验以及一些知识，但AI这块，我仅仅之前半途而废地上过一点网课…</p></blockquote><h2 id="开始之前">开始之前</h2><p>根据个人经验，在开始作业之前，最好是先学习课程知识和相关的Python知识（如果没有的话），这样做作业会更加高效。</p><p>课程知识的话，首先是<a href="https://cs231n.stanford.edu">官网</a>以及Youtube上的公开视频，B站上也会有人发。</p><p>相关的Python知识，在官网Schedule的2025/04/04<a href="https://cs231n.github.io/python-numpy-tutorial/">Tutorial</a>中，也可以在<a href="https://colab.research.google.com/github/cs231n/cs231n.github.io/blob/master/python-colab.ipynb">Colab</a>学习。最好在做作业1之前学习一下，不然对小白来说，很可能会像我一样，即使有Python基础，做个作业被Numpy什么的搞得一头雾水（比如知道该怎么实现，但一些Numpy的方法或者原理不清楚）。</p><p>作业的一些细节都能在官网找到，这里就不赘述了。</p><p>顺便分享一下我的仓库：<a href="https://github.com/logicff/cs231n">https://github.com/logicff/cs231n</a></p><h2 id="q1-k-nearest-neighbor-classifier">Q1: k-Nearest Neighbor classifier</h2><p>这部分将在<strong>knn.ipynb</strong>中实现一个kNN图像分类器。</p><p>kNN分类器由两个阶段组成：</p><ul><li>在训练阶段，分类器获取训练数据并直接存储</li><li>在测试阶段，kNN对每个测试图像进行分类时，会将其与所有训练图像进行对比，然后采用k个最相似训练样本的标签中出现次数最多的标签作为该测试图像的标签</li><li>k值通过交叉验证确定</li></ul><h3 id="数据加载处理和训练阶段">数据加载、处理和训练阶段</h3><p>运行完初始化单元格后，导入数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Load the raw CIFAR-10 data.</span><br>cifar10_dir = <span class="hljs-string">&#x27;cs231n/datasets/cifar-10-batches-py&#x27;</span><br><br><span class="hljs-comment"># Cleaning up variables to prevent loading data multiple times (which may cause memory issue)</span><br><span class="hljs-keyword">try</span>:<br>   <span class="hljs-keyword">del</span> X_train, y_train<br>   <span class="hljs-keyword">del</span> X_test, y_test<br>   <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Clear previously loaded data.&#x27;</span>)<br><span class="hljs-keyword">except</span>:<br>   <span class="hljs-keyword">pass</span><br><br>X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)<br><br><span class="hljs-comment"># As a sanity check, we print out the size of the training and test data.</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Training data shape: &#x27;</span>, X_train.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Training labels shape: &#x27;</span>, y_train.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Test data shape: &#x27;</span>, X_test.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Test labels shape: &#x27;</span>, y_test.shape)<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># Training data shape:  (50000, 32, 32, 3)</span><br><span class="hljs-comment"># Training labels shape:  (50000,)</span><br><span class="hljs-comment"># Test data shape:  (10000, 32, 32, 3)</span><br><span class="hljs-comment"># Test labels shape:  (10000,)</span><br></code></pre></td></tr></table></figure><p>展示一些训练数据样本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Visualize some examples from the dataset.</span><br><span class="hljs-comment"># We show a few examples of training images from each class.</span><br>classes = [<span class="hljs-string">&#x27;plane&#x27;</span>, <span class="hljs-string">&#x27;car&#x27;</span>, <span class="hljs-string">&#x27;bird&#x27;</span>, <span class="hljs-string">&#x27;cat&#x27;</span>, <span class="hljs-string">&#x27;deer&#x27;</span>, <span class="hljs-string">&#x27;dog&#x27;</span>, <span class="hljs-string">&#x27;frog&#x27;</span>, <span class="hljs-string">&#x27;horse&#x27;</span>, <span class="hljs-string">&#x27;ship&#x27;</span>, <span class="hljs-string">&#x27;truck&#x27;</span>]<br>num_classes = <span class="hljs-built_in">len</span>(classes)<br>samples_per_class = <span class="hljs-number">7</span><br><span class="hljs-keyword">for</span> y, cls <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(classes):<br>    idxs = np.flatnonzero(y_train == y)<br>    idxs = np.random.choice(idxs, samples_per_class, replace=<span class="hljs-literal">False</span>)<br>    <span class="hljs-keyword">for</span> i, idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(idxs):<br>        plt_idx = i * num_classes + y + <span class="hljs-number">1</span><br>        plt.subplot(samples_per_class, num_classes, plt_idx)<br>        plt.imshow(X_train[idx].astype(<span class="hljs-string">&#x27;uint8&#x27;</span>))<br>        plt.axis(<span class="hljs-string">&#x27;off&#x27;</span>)<br>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span>:<br>            plt.title(cls)<br>plt.show()<br></code></pre></td></tr></table></figure><p>为了测试效率，仅取训练集前5000个作为当前训练集，取测试集前500个作为当前测试集，并将图像展平为一个行向量，展平后的列数为32×32×3=3072列。：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Subsample the data for more efficient code execution in this exercise</span><br>num_training = <span class="hljs-number">5000</span><br>mask = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(num_training))<br>X_train = X_train[mask]<br>y_train = y_train[mask]<br><br>num_test = <span class="hljs-number">500</span><br>mask = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(num_test))<br>X_test = X_test[mask]<br>y_test = y_test[mask]<br><br><span class="hljs-comment"># Reshape the image data into rows</span><br>X_train = np.reshape(X_train, (X_train.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>))<br>X_test = np.reshape(X_test, (X_test.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>))<br><span class="hljs-built_in">print</span>(X_train.shape, X_test.shape)<br><br><span class="hljs-comment"># Output</span><br><span class="hljs-comment"># (5000, 3072) (500, 3072)</span><br></code></pre></td></tr></table></figure><p>导入kNN分类器，并存储训练数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> cs231n.classifiers <span class="hljs-keyword">import</span> KNearestNeighbor<br><br><span class="hljs-comment"># Create a kNN classifier instance.</span><br><span class="hljs-comment"># Remember that training a kNN classifier is a noop:</span><br><span class="hljs-comment"># the Classifier simply remembers the data and does no further processing</span><br>classifier = KNearestNeighbor()<br>classifier.train(X_train, y_train)<br></code></pre></td></tr></table></figure><h3 id="测试阶段">测试阶段</h3><p>计算每张测试图片与每张训练图片的距离：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Open cs231n/classifiers/k_nearest_neighbor.py and implement</span><br><span class="hljs-comment"># compute_distances_two_loops.</span><br><br><span class="hljs-comment"># Test your implementation:</span><br>dists = classifier.compute_distances_two_loops(X_test)<br><span class="hljs-built_in">print</span>(dists.shape)<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># (500, 5000)</span><br></code></pre></td></tr></table></figure><p>在<code>cs231n/classifiers/k_nearest_neighbor.py</code>中实现<code>compute_distances_two_loops</code>，直接套公式即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># In cs231n/classifiers/k_nearest_neighbor.py</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_distances_two_loops</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Compute the distance between each test point in X and each training point</span><br><span class="hljs-string">        in self.X_train using a nested loop over both the training data and the</span><br><span class="hljs-string">        test data.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - X: A numpy array of shape (num_test, D) containing test data.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span><br><span class="hljs-string">          is the Euclidean distance between the ith test point and the jth training</span><br><span class="hljs-string">          point.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        num_test = X.shape[<span class="hljs-number">0</span>]<br>        num_train = <span class="hljs-variable language_">self</span>.X_train.shape[<span class="hljs-number">0</span>]<br>        dists = np.zeros((num_test, num_train))<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_test):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_train):<br>                <span class="hljs-comment">#####################################################################</span><br>                <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                             #</span><br>                <span class="hljs-comment"># Compute the l2 distance between the ith test point and the jth    #</span><br>                <span class="hljs-comment"># training point, and store the result in dists[i, j]. You should   #</span><br>                <span class="hljs-comment"># not use a loop over dimension, nor use np.linalg.norm().          #</span><br>                <span class="hljs-comment">#####################################################################</span><br>                <span class="hljs-comment"># L2 distance: d2(I1, I2) = \sqrt&#123;\sum&#123;\square&#123;I^p_1 - I^p_2&#125;&#125;&#125;</span><br>                dists[i, j] = np.sqrt(np.<span class="hljs-built_in">sum</span>(np.square(X[i] - <span class="hljs-variable language_">self</span>.X_train[j])))<br>        <span class="hljs-keyword">return</span> dists<br></code></pre></td></tr></table></figure><p>回到knn.ipynb，可视化得到的<code>dists</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># We can visualize the distance matrix: each row is a single test example and</span><br><span class="hljs-comment"># its distances to training examples</span><br>plt.imshow(dists, interpolation=<span class="hljs-string">&#x27;none&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><figure><img src="/images/cs231n/dists-visual.png" alt="visualized distance matrix" /><figcaption>visualized distance matrix</figcaption></figure><p><strong>Inline Question 1</strong></p><p>Notice the structured patterns in the distance matrix, where some rows or columns are visibly brighter. (Note that with the default color scheme black indicates low distances while white indicates high distances.)</p><ul><li>What in the data is the cause behind the distinctly bright rows?</li><li>What causes the columns?</li></ul><p><span class="math inline">$\color{blue}{\textit Your Answer:}$</span></p><ul><li><p>Bright rows correspond to test samples that are dissimilar to all training samples. This happens because these test samples may be have high noise, or belong to classes with weak representation in the training set—leading to large distances to all stored training samples.</p></li><li><p>Bright columns correspond to training samples that are dissimilar to all test samples.</p></li></ul><p>第二个任务点，预测标签，输入一个距离矩阵，和最近邻居数量k，输出测试集的预测标签。先在<code>cs231n/classifiers/k_nearest_neighbor.py</code>中实现<code>predict_labels</code>，按照注释中的提示做即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># In cs231n/classifiers/k_nearest_neighbor.py</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict_labels</span>(<span class="hljs-params">self, dists, k=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Given a matrix of distances between test points and training points,</span><br><span class="hljs-string">        predict a label for each test point.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span><br><span class="hljs-string">          gives the distance betwen the ith test point and the jth training point.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">        - y: A numpy array of shape (num_test,) containing predicted labels for the</span><br><span class="hljs-string">          test data, where y[i] is the predicted label for the test point X[i].</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        num_test = dists.shape[<span class="hljs-number">0</span>]<br>        y_pred = np.zeros(num_test)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_test):<br>            <span class="hljs-comment"># A list of length k storing the labels of the k nearest neighbors to</span><br>            <span class="hljs-comment"># the ith test point.</span><br>            closest_y = []<br>            <span class="hljs-comment">#########################################################################</span><br>            <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                 #</span><br>            <span class="hljs-comment"># Use the distance matrix to find the k nearest neighbors of the ith    #</span><br>            <span class="hljs-comment"># testing point, and use self.y_train to find the labels of these       #</span><br>            <span class="hljs-comment"># neighbors. Store these labels in closest_y.                           #</span><br>            <span class="hljs-comment"># Hint: Look up the function numpy.argsort.                             #</span><br>            <span class="hljs-comment">#########################################################################</span><br>            k_nearest_idxs = np.argsort(dists[i])[:k]<br>            closest_y = <span class="hljs-variable language_">self</span>.y_train[k_nearest_idxs]<br><br><br>            <span class="hljs-comment">#########################################################################</span><br>            <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                 #</span><br>            <span class="hljs-comment"># Now that you have found the labels of the k nearest neighbors, you    #</span><br>            <span class="hljs-comment"># need to find the most common label in the list closest_y of labels.   #</span><br>            <span class="hljs-comment"># Store this label in y_pred[i]. Break ties by choosing the smaller     #</span><br>            <span class="hljs-comment"># label.                                                                #</span><br>            <span class="hljs-comment">#########################################################################</span><br>            counts = np.bincount(closest_y)<br>            y_pred[i] = np.argmax(counts)<br><br><br>        <span class="hljs-keyword">return</span> y_pred<br></code></pre></td></tr></table></figure><p>使用k=1进行预测，查看预测准确率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Now implement the function predict_labels and run the code below:</span><br><span class="hljs-comment"># We use k = 1 (which is Nearest Neighbor).</span><br>y_test_pred = classifier.predict_labels(dists, k=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># Compute and print the fraction of correctly predicted examples</span><br>num_correct = np.<span class="hljs-built_in">sum</span>(y_test_pred == y_test)<br>accuracy = <span class="hljs-built_in">float</span>(num_correct) / num_test<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Got %d / %d correct =&gt; accuracy: %f&#x27;</span> % (num_correct, num_test, accuracy))<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># Got 137 / 500 correct =&gt; accuracy: 0.274000</span><br></code></pre></td></tr></table></figure><p>再试试k=5：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">y_test_pred = classifier.predict_labels(dists, k=<span class="hljs-number">5</span>)<br>num_correct = np.<span class="hljs-built_in">sum</span>(y_test_pred == y_test)<br>accuracy = <span class="hljs-built_in">float</span>(num_correct) / num_test<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Got %d / %d correct =&gt; accuracy: %f&#x27;</span> % (num_correct, num_test, accuracy))<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># Got 139 / 500 correct =&gt; accuracy: 0.278000</span><br></code></pre></td></tr></table></figure><p>可以看到k=5的表现要比k=1稍微好一点。</p><p><strong>Inline Question 2</strong></p><p>We can also use other distance metrics such as L1 distance. For pixel values <span class="math inline"><em>p</em><sub><em>i</em><em>j</em></sub><sup>(<em>k</em>)</sup></span> at location <span class="math inline">(<em>i</em>, <em>j</em>)</span> of some image <span class="math inline"><em>I</em><sub><em>k</em></sub></span>,</p><p>the mean <span class="math inline"><em>μ</em></span> across all pixels over all images is <br /><span class="math display">$$\mu=\frac{1}{nhw}\sum_{k=1}^n\sum_{i=1}^{h}\sum_{j=1}^{w}p_{ij}^{(k)}$$</span><br /> And the pixel-wise mean <span class="math inline"><em>μ</em><sub><em>i</em><em>j</em></sub></span> across all images is <br /><span class="math display">$$\mu_{ij}=\frac{1}{n}\sum_{k=1}^np_{ij}^{(k)}.$$</span><br /> The general standard deviation <span class="math inline"><em>σ</em></span> and pixel-wise standard deviation <span class="math inline"><em>σ</em><sub><em>i</em><em>j</em></sub></span> is defined similarly.</p><p>Which of the following preprocessing steps will not change the performance of a Nearest Neighbor classifier that uses L1 distance? Select all that apply. To clarify, both training and test examples are preprocessed in the same way.</p><ol type="1"><li>Subtracting the mean <span class="math inline"><em>μ</em></span> (<span class="math inline"><em>p̃</em><sub><em>i</em><em>j</em></sub><sup>(<em>k</em>)</sup> = <em>p</em><sub><em>i</em><em>j</em></sub><sup>(<em>k</em>)</sup> − <em>μ</em></span>.)</li><li>Subtracting the per pixel mean <span class="math inline"><em>μ</em><sub><em>i</em><em>j</em></sub></span> (<span class="math inline"><em>p̃</em><sub><em>i</em><em>j</em></sub><sup>(<em>k</em>)</sup> = <em>p</em><sub><em>i</em><em>j</em></sub><sup>(<em>k</em>)</sup> − <em>μ</em><sub><em>i</em><em>j</em></sub></span>.)</li><li>Subtracting the mean <span class="math inline"><em>μ</em></span> and dividing by the standard deviation <span class="math inline"><em>σ</em></span>.</li><li>Subtracting the pixel-wise mean <span class="math inline"><em>μ</em><sub><em>i</em><em>j</em></sub></span> and dividing by the pixel-wise standard deviation <span class="math inline"><em>σ</em><sub><em>i</em><em>j</em></sub></span>.</li><li>Rotating the coordinate axes of the data, which means rotating all the images by the same angle. Empty regions in the image caused by rotation are padded with a same pixel value and no interpolation is performed.</li></ol><p><span class="math inline">$\color{blue}{\textit Your Answer:}$</span> 1, 2, 3, 5</p><p><span class="math inline">$\color{blue}{\textit Your Explanation:}$</span> The core principle is that preprocessing must preserve the relative magnitudes of L1 distances between samples. Only 4 breaks it.</p><p>现在尝试在一个循环内完成<code>dists</code>的计算，即每次循环直接计算一个测试图像与所有训练图像的距离，可以利用Numpy的广播机制（开始提到的<a href="https://cs231n.github.io/python-numpy-tutorial/">Tutorial</a>中有讲）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># In cs231n/classifiers/k_nearest_neighbor.py</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_distances_one_loop</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Compute the distance between each test point in X and each training point</span><br><span class="hljs-string">        in self.X_train using a single loop over the test data.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Input / Output: Same as compute_distances_two_loops</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        num_test = X.shape[<span class="hljs-number">0</span>]<br>        num_train = <span class="hljs-variable language_">self</span>.X_train.shape[<span class="hljs-number">0</span>]<br>        dists = np.zeros((num_test, num_train))<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_test):<br>            <span class="hljs-comment">#######################################################################</span><br>            <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                               #</span><br>            <span class="hljs-comment"># Compute the l2 distance between the ith test point and all training #</span><br>            <span class="hljs-comment"># points, and store the result in dists[i, :].                        #</span><br>            <span class="hljs-comment"># Do not use np.linalg.norm().                                        #</span><br>            <span class="hljs-comment">#######################################################################</span><br>            dists[i] = np.sqrt(np.<span class="hljs-built_in">sum</span>(np.square(X[i] - <span class="hljs-variable language_">self</span>.X_train), axis=<span class="hljs-number">1</span>))<br>        <span class="hljs-keyword">return</span> dists<br></code></pre></td></tr></table></figure><p>测试并检查正确性：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Now lets speed up distance matrix computation by using partial vectorization</span><br><span class="hljs-comment"># with one loop. Implement the function compute_distances_one_loop and run the</span><br><span class="hljs-comment"># code below:</span><br>dists_one = classifier.compute_distances_one_loop(X_test)<br><br><span class="hljs-comment"># To ensure that our vectorized implementation is correct, we make sure that it</span><br><span class="hljs-comment"># agrees with the naive implementation. There are many ways to decide whether</span><br><span class="hljs-comment"># two matrices are similar; one of the simplest is the Frobenius norm. In case</span><br><span class="hljs-comment"># you haven&#x27;t seen it before, the Frobenius norm of two matrices is the square</span><br><span class="hljs-comment"># root of the squared sum of differences of all elements; in other words, reshape</span><br><span class="hljs-comment"># the matrices into vectors and compute the Euclidean distance between them.</span><br>difference = np.linalg.norm(dists - dists_one, <span class="hljs-built_in">ord</span>=<span class="hljs-string">&#x27;fro&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;One loop difference was: %f&#x27;</span> % (difference, ))<br><span class="hljs-keyword">if</span> difference &lt; <span class="hljs-number">0.001</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Good! The distance matrices are the same&#x27;</span>)<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Uh-oh! The distance matrices are different&#x27;</span>)<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># One loop difference was: 0.000000</span><br><span class="hljs-comment"># Good! The distance matrices are the same</span><br></code></pre></td></tr></table></figure><p>接下来，再尝试无循环计算<code>dists</code>，这次要和之前的two_loop、one_loop有本质上的不同。</p><p>尽管one_loop借助了Numpy的广播机制，但在计算量和计算效率上和two_loop差别不大，可以说one_loop和two_loop没有本质上的差别。</p><p>首先，我们看距离公式： <br /><span class="math display"><em>D</em><sub><em>i</em>, <em>j</em></sub> = (<em>t</em><em>e</em><em>s</em><em>t</em><sub><em>i</em></sub> − <em>t</em><em>r</em><em>a</em><em>i</em><em>n</em><sub><em>j</em></sub>)<sup>2</sup> = <em>t</em><em>e</em><em>s</em><em>t</em><sub><em>i</em></sub><sup>2</sup> + <em>t</em><em>r</em><em>a</em><em>i</em><em>n</em><sub><em>j</em></sub><sup>2</sup> − 2<em>t</em><em>e</em><em>s</em><em>t</em><sub><em>i</em></sub> * <em>t</em><em>r</em><em>a</em><em>i</em><em>n</em><sub><em>j</em></sub></span><br /> 展开之前，想要不用循环的话，可以借助Numpy的广播机制，如<code>dists = np.sqrt(np.sum(np.square(X[:, np.newaxis, :] - self.X_train), axis=2))</code>，或者<code>dists = np.sqrt(np.sum(np.square(X[:, np.newaxis, :] - self.X_train[np.newaxis, :, :]), axis=2))</code>，但是这个方法，一是和之前的one_loop和two_loop没有本质上的差别（除非有底层优化），二是内存占用也是相当大的（拿作业的数据来说，<code>X[:, np.newaxis, :] - self.X_train[np.newaxis, :, :]</code>将得到一个包含num_test×num_train×3072个元素的大数组/矩阵）。</p><p>展开之后，两个平方项可以不用多次计算，分别计算后保存，后续复用这些结果即可，有点像动态规划的思想，而展开前的平方项由于同时依赖test_i和train_j，就没法这么做了。除此之外，test_i * train_j则可借助矩阵乘法实现，虽然计算量没变，但矩阵乘法是有底层优化的，运行速度很快。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># In cs231n/classifiers/k_nearest_neighbor.py</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_distances_no_loops</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Compute the distance between each test point in X and each training point</span><br><span class="hljs-string">        in self.X_train using no explicit loops.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Input / Output: Same as compute_distances_two_loops</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        num_test = X.shape[<span class="hljs-number">0</span>]<br>        num_train = <span class="hljs-variable language_">self</span>.X_train.shape[<span class="hljs-number">0</span>]<br>        dists = np.zeros((num_test, num_train))<br>        <span class="hljs-comment">#########################################################################</span><br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                 #</span><br>        <span class="hljs-comment"># Compute the l2 distance between all test points and all training      #</span><br>        <span class="hljs-comment"># points without using any explicit loops, and store the result in      #</span><br>        <span class="hljs-comment"># dists.                                                                #</span><br>        <span class="hljs-comment">#                                                                       #</span><br>        <span class="hljs-comment"># You should implement this function using only basic array operations; #</span><br>        <span class="hljs-comment"># in particular you should not use functions from scipy,                #</span><br>        <span class="hljs-comment"># nor use np.linalg.norm().                                             #</span><br>        <span class="hljs-comment">#                                                                       #</span><br>        <span class="hljs-comment"># HINT: Try to formulate the l2 distance using matrix multiplication    #</span><br>        <span class="hljs-comment">#       and two broadcast sums.                                         #</span><br>        <span class="hljs-comment">#########################################################################</span><br>        sum_x2 = np.reshape(np.<span class="hljs-built_in">sum</span>(np.square(X), axis=<span class="hljs-number">1</span>), (num_test, <span class="hljs-number">1</span>))<br>        sum_xt2 = np.reshape(np.<span class="hljs-built_in">sum</span>(np.square(<span class="hljs-variable language_">self</span>.X_train), axis=<span class="hljs-number">1</span>), (<span class="hljs-number">1</span>, num_train))<br>        matmul_xxt = np.dot(X, <span class="hljs-variable language_">self</span>.X_train.T)<br>        dists = np.sqrt(sum_x2 + sum_xt2 - <span class="hljs-number">2</span> * matmul_xxt)<br>        <span class="hljs-keyword">return</span> dists<br></code></pre></td></tr></table></figure><p>测试并检查正确性：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Now implement the fully vectorized version inside compute_distances_no_loops</span><br><span class="hljs-comment"># and run the code</span><br>dists_two = classifier.compute_distances_no_loops(X_test)<br><br><span class="hljs-comment"># check that the distance matrix agrees with the one we computed before:</span><br>difference = np.linalg.norm(dists - dists_two, <span class="hljs-built_in">ord</span>=<span class="hljs-string">&#x27;fro&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;No loop difference was: %f&#x27;</span> % (difference, ))<br><span class="hljs-keyword">if</span> difference &lt; <span class="hljs-number">0.001</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Good! The distance matrices are the same&#x27;</span>)<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Uh-oh! The distance matrices are different&#x27;</span>)<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># No loop difference was: 0.000000</span><br><span class="hljs-comment"># Good! The distance matrices are the same</span><br></code></pre></td></tr></table></figure><p>三种方法的运行时间对比：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Let&#x27;s compare how fast the implementations are</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">time_function</span>(<span class="hljs-params">f, *args</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Call a function f with args and return the time (in seconds) that it took to execute.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">import</span> time<br>    tic = time.time()<br>    f(*args)<br>    toc = time.time()<br>    <span class="hljs-keyword">return</span> toc - tic<br><br>two_loop_time = time_function(classifier.compute_distances_two_loops, X_test)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Two loop version took %f seconds&#x27;</span> % two_loop_time)<br><br>one_loop_time = time_function(classifier.compute_distances_one_loop, X_test)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;One loop version took %f seconds&#x27;</span> % one_loop_time)<br><br>no_loop_time = time_function(classifier.compute_distances_no_loops, X_test)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;No loop version took %f seconds&#x27;</span> % no_loop_time)<br><br><span class="hljs-comment"># You should see significantly faster performance with the fully vectorized implementation!</span><br><br><span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> depending on what machine you&#x27;re using,</span><br><span class="hljs-comment"># you might not see a speedup when you go from two loops to one loop,</span><br><span class="hljs-comment"># and might even see a slow-down.</span><br><br><span class="hljs-comment"># my Output:</span><br><span class="hljs-comment"># Two loop version took 39.707868 seconds</span><br><span class="hljs-comment"># One loop version took 61.474561 seconds</span><br><span class="hljs-comment"># No loop version took 0.530087 seconds</span><br><span class="hljs-comment"># 说明：运行到one_loop时和Colab连接断开，一会后才继续恢复执行，所以时间有点离谱</span><br><span class="hljs-comment"># 不过跑前面的的代码块的时候one_loop的时间大概45秒左右（没记错的话）。</span><br></code></pre></td></tr></table></figure><h3 id="交叉验证">交叉验证</h3><p>我们已经实现了kNN分类器，但目前的k=5是随意设定的。我们将通过交叉验证取得最好的超参数值（k值）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python">num_folds = <span class="hljs-number">5</span><br>k_choices = [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">8</span>, <span class="hljs-number">10</span>, <span class="hljs-number">12</span>, <span class="hljs-number">15</span>, <span class="hljs-number">20</span>, <span class="hljs-number">50</span>, <span class="hljs-number">100</span>]<br><br>X_train_folds = []<br>y_train_folds = []<br><span class="hljs-comment">################################################################################</span><br><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                        #</span><br><span class="hljs-comment"># Split up the training data into folds. After splitting, X_train_folds and    #</span><br><span class="hljs-comment"># y_train_folds should each be lists of length num_folds, where                #</span><br><span class="hljs-comment"># y_train_folds[i] is the label vector for the points in X_train_folds[i].     #</span><br><span class="hljs-comment"># Hint: Look up the numpy array_split function.                                #</span><br><span class="hljs-comment">################################################################################</span><br>X_train_folds = np.array_split(X_train, num_folds)<br>y_train_folds = np.array_split(y_train, num_folds)<br><br><span class="hljs-comment"># A dictionary holding the accuracies for different values of k that we find</span><br><span class="hljs-comment"># when running cross-validation. After running cross-validation,</span><br><span class="hljs-comment"># k_to_accuracies[k] should be a list of length num_folds giving the different</span><br><span class="hljs-comment"># accuracy values that we found when using that value of k.</span><br>k_to_accuracies = &#123;&#125;<br><br><br><span class="hljs-comment">################################################################################</span><br><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                        #</span><br><span class="hljs-comment"># Perform k-fold cross validation to find the best value of k. For each        #</span><br><span class="hljs-comment"># possible value of k, run the k-nearest-neighbor algorithm num_folds times,   #</span><br><span class="hljs-comment"># where in each case you use all but one of the folds as training data and the #</span><br><span class="hljs-comment"># last fold as a validation set. Store the accuracies for all fold and all     #</span><br><span class="hljs-comment"># values of k in the k_to_accuracies dictionary.                               #</span><br><span class="hljs-comment">################################################################################</span><br><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> k_choices:<br>  k_to_accuracies[k] = []<br>  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_folds):<br>    train_X = np.concatenate([X_train_folds[j] <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_folds) <span class="hljs-keyword">if</span> j != i])<br>    train_y = np.concatenate([y_train_folds[j] <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_folds) <span class="hljs-keyword">if</span> j != i])<br>    test_X = X_train_folds[i]<br>    test_y = y_train_folds[i]<br>    classifier = KNearestNeighbor()<br>    classifier.train(train_X, train_y)<br>    test_y_pred = classifier.predict(X=test_X, k=k, num_loops=<span class="hljs-number">0</span>)<br>    num_correct = np.<span class="hljs-built_in">sum</span>(test_y_pred == test_y)<br>    accuracy = <span class="hljs-built_in">float</span>(num_correct) / <span class="hljs-built_in">len</span>(test_y)<br>    k_to_accuracies[k].append(accuracy)<br><br><span class="hljs-comment"># Print out the computed accuracies</span><br><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(k_to_accuracies):<br>    <span class="hljs-keyword">for</span> accuracy <span class="hljs-keyword">in</span> k_to_accuracies[k]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;k = %d, accuracy = %f&#x27;</span> % (k, accuracy))<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs output">k = 1, accuracy = 0.263000<br>k = 1, accuracy = 0.257000<br>k = 1, accuracy = 0.264000<br>k = 1, accuracy = 0.278000<br>k = 1, accuracy = 0.266000<br>k = 3, accuracy = 0.239000<br>k = 3, accuracy = 0.249000<br>k = 3, accuracy = 0.240000<br>k = 3, accuracy = 0.266000<br>k = 3, accuracy = 0.254000<br>k = 5, accuracy = 0.248000<br>k = 5, accuracy = 0.266000<br>k = 5, accuracy = 0.280000<br>k = 5, accuracy = 0.292000<br>k = 5, accuracy = 0.280000<br>k = 8, accuracy = 0.262000<br>k = 8, accuracy = 0.282000<br>k = 8, accuracy = 0.273000<br>k = 8, accuracy = 0.290000<br>k = 8, accuracy = 0.273000<br>k = 10, accuracy = 0.265000<br>k = 10, accuracy = 0.296000<br>k = 10, accuracy = 0.276000<br>k = 10, accuracy = 0.284000<br>k = 10, accuracy = 0.280000<br>k = 12, accuracy = 0.260000<br>k = 12, accuracy = 0.295000<br>k = 12, accuracy = 0.279000<br>k = 12, accuracy = 0.283000<br>k = 12, accuracy = 0.280000<br>k = 15, accuracy = 0.252000<br>k = 15, accuracy = 0.289000<br>k = 15, accuracy = 0.278000<br>k = 15, accuracy = 0.282000<br>k = 15, accuracy = 0.274000<br>k = 20, accuracy = 0.270000<br>k = 20, accuracy = 0.279000<br>k = 20, accuracy = 0.279000<br>k = 20, accuracy = 0.282000<br>k = 20, accuracy = 0.285000<br>k = 50, accuracy = 0.271000<br>k = 50, accuracy = 0.288000<br>k = 50, accuracy = 0.278000<br>k = 50, accuracy = 0.269000<br>k = 50, accuracy = 0.266000<br>k = 100, accuracy = 0.256000<br>k = 100, accuracy = 0.270000<br>k = 100, accuracy = 0.263000<br>k = 100, accuracy = 0.256000<br>k = 100, accuracy = 0.263000<br></code></pre></td></tr></table></figure><p>可视化一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># plot the raw observations</span><br><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> k_choices:<br>    accuracies = k_to_accuracies[k]<br>    plt.scatter([k] * <span class="hljs-built_in">len</span>(accuracies), accuracies)<br><br><span class="hljs-comment"># plot the trend line with error bars that correspond to standard deviation</span><br>accuracies_mean = np.array([np.mean(v) <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(k_to_accuracies.items())])<br>accuracies_std = np.array([np.std(v) <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(k_to_accuracies.items())])<br>plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)<br>plt.title(<span class="hljs-string">&#x27;Cross-validation on k&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;k&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Cross-validation accuracy&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><figure><img src="/images/cs231n/knn-cross-val.png" alt="Cross-validation on k" /><figcaption>Cross-validation on k</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Based on the cross-validation results above, choose the best value for k,</span><br><span class="hljs-comment"># retrain the classifier using all the training data, and test it on the test</span><br><span class="hljs-comment"># data. You should be able to get above 28% accuracy on the test data.</span><br>best_k = <span class="hljs-number">10</span><br><br>classifier = KNearestNeighbor()<br>classifier.train(X_train, y_train)<br>y_test_pred = classifier.predict(X_test, k=best_k)<br><br><span class="hljs-comment"># Compute and display the accuracy</span><br>num_correct = np.<span class="hljs-built_in">sum</span>(y_test_pred == y_test)<br>accuracy = <span class="hljs-built_in">float</span>(num_correct) / num_test<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Got %d / %d correct =&gt; accuracy: %f&#x27;</span> % (num_correct, num_test, accuracy))<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># Got 141 / 500 correct =&gt; accuracy: 0.282000</span><br></code></pre></td></tr></table></figure><p><strong>Inline Question 3</strong></p><p>Which of the following statements about <span class="math inline"><em>k</em></span>-Nearest Neighbor (<span class="math inline"><em>k</em></span>-NN) are true in a classification setting, and for all <span class="math inline"><em>k</em></span>? Select all that apply. 1. The decision boundary of the k-NN classifier is linear. 2. The training error of a 1-NN will always be lower than or equal to that of 5-NN. 3. The test error of a 1-NN will always be lower than that of a 5-NN. 4. The time needed to classify a test example with the k-NN classifier grows with the size of the training set. 5. None of the above.</p><p><span class="math inline">$\color{blue}{\textit Your Answer:}$</span> 2, 4</p><p><span class="math inline">$\color{blue}{\textit Your Explanation:}$</span> When k=1, each prediction is based on solely point, which may lead to overfitting, when increasing k, the classifier gain the ability of generalization, it perform on training may not as good as 1-NN. For k-NN, all computations are in prediction, so once test a example, it needs to calculate distances to all training points.</p><h2 id="q2-implement-a-softmax-classifier">Q2: Implement a Softmax classifier</h2><p>这部分将在<strong>softmax.ipynb</strong>中实现一个Softmax图像分类器，会涉及到正则化和优化。</p><p>从现在开始，将不会对与之前一样或者相似的步骤进行详细记录。</p><h3 id="数据预处理">数据预处理</h3><p>运行初始化和导入数据单元格后，对数据进行预处理。首先，划分训练集和验证集，除此之外，还有一个训练数据的小子集用于提高运行效率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Split the data into train, val, and test sets. In addition we will</span><br><span class="hljs-comment"># create a small development set as a subset of the training data;</span><br><span class="hljs-comment"># we can use this for development so our code runs faster.</span><br>num_training = <span class="hljs-number">49000</span><br>num_validation = <span class="hljs-number">1000</span><br>num_test = <span class="hljs-number">1000</span><br>num_dev = <span class="hljs-number">500</span><br><br><span class="hljs-comment"># Our validation set will be num_validation points from the original</span><br><span class="hljs-comment"># training set.</span><br>mask = <span class="hljs-built_in">range</span>(num_training, num_training + num_validation)<br>X_val = X_train[mask]<br>y_val = y_train[mask]<br><br><span class="hljs-comment"># Our training set will be the first num_train points from the original</span><br><span class="hljs-comment"># training set.</span><br>mask = <span class="hljs-built_in">range</span>(num_training)<br>X_train = X_train[mask]<br>y_train = y_train[mask]<br><br><span class="hljs-comment"># We will also make a development set, which is a small subset of</span><br><span class="hljs-comment"># the training set.</span><br>mask = np.random.choice(num_training, num_dev, replace=<span class="hljs-literal">False</span>)<br>X_dev = X_train[mask]<br>y_dev = y_train[mask]<br><br><span class="hljs-comment"># We use the first num_test points of the original test set as our</span><br><span class="hljs-comment"># test set.</span><br>mask = <span class="hljs-built_in">range</span>(num_test)<br>X_test = X_test[mask]<br>y_test = y_test[mask]<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Train data shape: &#x27;</span>, X_train.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Train labels shape: &#x27;</span>, y_train.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Validation data shape: &#x27;</span>, X_val.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Validation labels shape: &#x27;</span>, y_val.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Test data shape: &#x27;</span>, X_test.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Test labels shape: &#x27;</span>, y_test.shape)<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># Train data shape:  (49000, 32, 32, 3)</span><br><span class="hljs-comment"># Train labels shape:  (49000,)</span><br><span class="hljs-comment"># Validation data shape:  (1000, 32, 32, 3)</span><br><span class="hljs-comment"># Validation labels shape:  (1000,)</span><br><span class="hljs-comment"># Test data shape:  (1000, 32, 32, 3)</span><br><span class="hljs-comment"># Test labels shape:  (1000,)</span><br></code></pre></td></tr></table></figure><p>接着将图像展平，然后减去均值图像并添加偏项，以优化模型的训练效果和计算效率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Preprocessing: subtract the mean image</span><br><span class="hljs-comment"># first: compute the image mean based on the training data</span><br>mean_image = np.mean(X_train, axis=<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(mean_image[:<span class="hljs-number">10</span>]) <span class="hljs-comment"># print a few of the elements</span><br>plt.figure(figsize=(<span class="hljs-number">4</span>,<span class="hljs-number">4</span>))<br>plt.imshow(mean_image.reshape((<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,<span class="hljs-number">3</span>)).astype(<span class="hljs-string">&#x27;uint8&#x27;</span>)) <span class="hljs-comment"># visualize the mean image</span><br>plt.show()<br><br><span class="hljs-comment"># second: subtract the mean image from train and test data</span><br>X_train -= mean_image<br>X_val -= mean_image<br>X_test -= mean_image<br>X_dev -= mean_image<br><br><span class="hljs-comment"># third: append the bias dimension of ones (i.e. bias trick) so that our classifier</span><br><span class="hljs-comment"># only has to worry about optimizing a single weight matrix W.</span><br>X_train = np.hstack([X_train, np.ones((X_train.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>))])<br>X_val = np.hstack([X_val, np.ones((X_val.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>))])<br>X_test = np.hstack([X_test, np.ones((X_test.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>))])<br>X_dev = np.hstack([X_dev, np.ones((X_dev.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>))])<br><br><span class="hljs-built_in">print</span>(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)<br><br><span class="hljs-comment"># Output1:</span><br><span class="hljs-comment"># [130.64189796 135.98173469 132.47391837 130.05569388 135.34804082</span><br><span class="hljs-comment">#  131.75402041 130.96055102 136.14328571 132.47636735 131.48467347]</span><br><br><span class="hljs-comment"># Output2(见下图)</span><br><br><span class="hljs-comment"># Output3:</span><br><span class="hljs-comment"># (49000, 3073) (1000, 3073) (1000, 3073) (500, 3073)</span><br></code></pre></td></tr></table></figure><figure><img src="/images/cs231n/mean-image.png" alt="the image mean based on the training data" /><figcaption>the image mean based on the training data</figcaption></figure><h3 id="训练和测试">训练和测试</h3><p>利用循环计算Softmax损失和梯度，损失函数： <br /><span class="math display">$$L = \frac{1}{N} \sum_{i=1}^N L_i + \lambda \sum_{k} W_k^2$$</span><br /> 第一项为交叉熵损失，第二项为L2正则项，<span class="math inline"><em>L</em><sub><em>i</em></sub></span> 为单个样本 <span class="math inline"><em>x</em><sub><em>i</em></sub></span> （真实标签为 <span class="math inline"><em>y</em><sub><em>i</em></sub></span>）的交叉熵损失，W 为权重矩阵，其中 <span class="math inline"><em>L</em><sub><em>i</em></sub></span> 为： <br /><span class="math display">$$L_i = -log(\frac{e^{s_{y_i}}}{\sum_{j} e^{s_j}}) = -s_{y_i} + log\sum_{j} e^{s_j}$$</span><br /> 得分函数 s 为样本 <span class="math inline"><em>x</em><sub><em>i</em></sub></span> 在各个类别上的得分： <br /><span class="math display"><em>s</em> = <em>x</em><sub><em>i</em></sub><em>W</em></span><br /> 因此，我们可以计算梯度： <br /><span class="math display">$$\nabla_{W}L = \frac{1}{N} \sum_{i=1}^N \nabla_{W}L_i + 2\lambda W$$</span><br /> 其中 <span class="math inline">∇<sub><em>W</em></sub><em>L</em><sub><em>i</em></sub></span> 可以通过链式法则计算： <br /><span class="math display">$$\nabla_{W}L_i = \frac{\partial L_i}{\partial s} \frac{\partial s}{\partial W}$$</span><br /> 但是直接计算 <span class="math inline">$\frac{\partial L_i}{\partial s}$</span> 似乎很难，我们可以改为计算 <span class="math inline">$\frac{\partial L_i}{\partial s_j}$</span>，其中 <span class="math inline"><em>s</em><sub><em>j</em></sub></span> 为样本 <span class="math inline"><em>x</em><sub><em>i</em></sub></span> 在第 j 类上的得分： <br /><span class="math display"><em>s</em><sub><em>j</em></sub> = <em>x</em><sub><em>i</em></sub><em>W</em><sub><em>j</em></sub></span><br /> <br /><span class="math display">$$\nabla_{W_{j}}L_i = \frac{\partial L_i}{\partial s_j} \frac{\partial s_j}{\partial W_j}$$</span><br /></p><blockquote><p>这里 <span class="math inline"><em>W</em><sub><em>j</em></sub></span> 表示在第 j 类上的权重，所以在后面的代码中是 W 的第 j 列而不是第 j 行。</p></blockquote><p>从元素组成上来看，很容易发现 <span class="math inline">∇<sub><em>W</em></sub><em>L</em><sub><em>i</em></sub></span> 由 <span class="math inline">∇<sub><em>W</em><sub><em>j</em></sub></sub><em>L</em><sub><em>i</em></sub></span> （j = 1, 2, …, num_classes）组成，这里 <span class="math inline">∇<sub><em>W</em></sub><em>L</em><sub><em>i</em></sub></span> 的第 j 列就是 <span class="math inline">∇<sub><em>W</em><sub><em>j</em></sub></sub><em>L</em><sub><em>i</em></sub></span> 。</p><p>根据上面的式子，容易得到： <br /><span class="math display">$$\frac{\partial L_i}{\partial s_j} = \begin{cases} -1 + \frac{e^{s_{j}}}{\sum_{k} e^{s_k}} &amp; \text{if } j = y_i, \\ \frac{e^{s_{j}}}{\sum_{k} e^{s_k}} &amp; \text{if } j \neq y_i. \end{cases}$$</span><br /> <br /><span class="math display">$$\frac{\partial s_j}{\partial W_j} = x_i^T$$</span><br /> 现在，解析梯度的计算就可以开始了，在代码中，会用 <span class="math inline">$p_j = \frac{e^{s_{j}}}{\sum_{k} e^{s_k}}$</span> 来稍微简化上面的式子，实现<code>cs231n/classifiers/softmax.py</code>中的<code>softmax_loss_naive</code>函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># In cs231n/classifiers/softmax.py</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax_loss_naive</span>(<span class="hljs-params">W, X, y, reg</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Softmax loss function, naive implementation (with loops)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs have dimension D, there are C classes, and we operate on minibatches</span><br><span class="hljs-string">    of N examples.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - W: A numpy array of shape (D, C) containing weights.</span><br><span class="hljs-string">    - X: A numpy array of shape (N, D) containing a minibatch of data.</span><br><span class="hljs-string">    - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span><br><span class="hljs-string">      that X[i] has label c, where 0 &lt;= c &lt; C.</span><br><span class="hljs-string">    - reg: (float) regularization strength</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns a tuple of:</span><br><span class="hljs-string">    - loss as single float</span><br><span class="hljs-string">    - gradient with respect to weights W; an array of same shape as W</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># Initialize the loss and gradient to zero.</span><br>    loss = <span class="hljs-number">0.0</span><br>    dW = np.zeros_like(W)<br><br>    <span class="hljs-comment"># compute the loss and the gradient</span><br>    num_classes = W.shape[<span class="hljs-number">1</span>]<br>    num_train = X.shape[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_train):<br>        scores = X[i].dot(W)<br><br>        <span class="hljs-comment"># compute the probabilities in numerically stable way</span><br>        scores -= np.<span class="hljs-built_in">max</span>(scores)<br>        p = np.exp(scores)<br>        p /= p.<span class="hljs-built_in">sum</span>()  <span class="hljs-comment"># normalize</span><br>        logp = np.log(p)<br><br>        loss -= logp[y[i]]  <span class="hljs-comment"># negative log probability is the loss</span><br>        <br>        <span class="hljs-comment"># compute the gradient</span><br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_classes):<br>            dW[:, j] += (p[j] - (j == y[i])) * X[i]<br><br>    <span class="hljs-comment"># normalized hinge loss plus regularization</span><br>    loss = loss / num_train + reg * np.<span class="hljs-built_in">sum</span>(W * W)<br><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                     #</span><br>    <span class="hljs-comment"># Compute the gradient of the loss function and store it dW.                #</span><br>    <span class="hljs-comment"># Rather that first computing the loss and then computing the derivative,   #</span><br>    <span class="hljs-comment"># it may be simpler to compute the derivative at the same time that the     #</span><br>    <span class="hljs-comment"># loss is being computed. As a result you may need to modify some of the    #</span><br>    <span class="hljs-comment"># code above to compute the gradient.                                       #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    dW = dW / num_train + <span class="hljs-number">2</span> * reg * W<br><br>    <span class="hljs-keyword">return</span> loss, dW<br></code></pre></td></tr></table></figure><p>测试损失和梯度的计算：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Evaluate the naive implementation of the loss we provided for you:</span><br><span class="hljs-keyword">from</span> cs231n.classifiers.softmax <span class="hljs-keyword">import</span> softmax_loss_naive<br><span class="hljs-keyword">import</span> time<br><br><span class="hljs-comment"># generate a random Softmax classifier weight matrix of small numbers</span><br>W = np.random.randn(<span class="hljs-number">3073</span>, <span class="hljs-number">10</span>) * <span class="hljs-number">0.0001</span><br><br>loss, grad = softmax_loss_naive(W, X_dev, y_dev, <span class="hljs-number">0.000005</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;loss: %f&#x27;</span> % (loss, ))<br><br><span class="hljs-comment"># As a rough sanity check, our loss should be something close to -log(0.1).</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;loss: %f&#x27;</span> % loss)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;sanity check: %f&#x27;</span> % (-np.log(<span class="hljs-number">0.1</span>)))<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># loss: 2.344003</span><br><span class="hljs-comment"># loss: 2.344003</span><br><span class="hljs-comment"># sanity check: 2.302585</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Once you&#x27;ve implemented the gradient, recompute it with the code below</span><br><span class="hljs-comment"># and gradient check it with the function we provided for you</span><br><br><span class="hljs-comment"># Compute the loss and its gradient at W.</span><br>loss, grad = softmax_loss_naive(W, X_dev, y_dev, <span class="hljs-number">0.0</span>)<br><br><span class="hljs-comment"># Numerically compute the gradient along several randomly chosen dimensions, and</span><br><span class="hljs-comment"># compare them with your analytically computed gradient. The numbers should match</span><br><span class="hljs-comment"># almost exactly along all dimensions.</span><br><span class="hljs-keyword">from</span> cs231n.gradient_check <span class="hljs-keyword">import</span> grad_check_sparse<br>f = <span class="hljs-keyword">lambda</span> w: softmax_loss_naive(w, X_dev, y_dev, <span class="hljs-number">0.0</span>)[<span class="hljs-number">0</span>]<br>grad_numerical = grad_check_sparse(f, W, grad)<br><br><span class="hljs-comment"># do the gradient check once again with regularization turned on</span><br><span class="hljs-comment"># you didn&#x27;t forget the regularization gradient did you?</span><br>loss, grad = softmax_loss_naive(W, X_dev, y_dev, <span class="hljs-number">5e1</span>)<br>f = <span class="hljs-keyword">lambda</span> w: softmax_loss_naive(w, X_dev, y_dev, <span class="hljs-number">5e1</span>)[<span class="hljs-number">0</span>]<br>grad_numerical = grad_check_sparse(f, W, grad)<br><br><span class="hljs-comment"># Output</span><br><span class="hljs-comment"># numerical: 0.637966 analytic: 0.637965, relative error: 1.236012e-07</span><br><span class="hljs-comment"># numerical: -0.354512 analytic: -0.354512, relative error: 1.075945e-08</span><br><span class="hljs-comment"># numerical: 0.111390 analytic: 0.111390, relative error: 1.455617e-07</span><br><span class="hljs-comment"># numerical: -1.616721 analytic: -1.616721, relative error: 8.005533e-08</span><br><span class="hljs-comment"># numerical: 1.509032 analytic: 1.509032, relative error: 2.254300e-08</span><br><span class="hljs-comment"># numerical: 1.250068 analytic: 1.250068, relative error: 7.258522e-08</span><br><span class="hljs-comment"># numerical: 3.575710 analytic: 3.575710, relative error: 2.251164e-08</span><br><span class="hljs-comment"># numerical: -2.220965 analytic: -2.220966, relative error: 1.706946e-08</span><br><span class="hljs-comment"># numerical: -0.745233 analytic: -0.745233, relative error: 4.451017e-08</span><br><span class="hljs-comment"># numerical: -4.452108 analytic: -4.452107, relative error: 6.863113e-09</span><br><span class="hljs-comment"># numerical: -0.437320 analytic: -0.437320, relative error: 4.286594e-08</span><br><span class="hljs-comment"># numerical: 2.110840 analytic: 2.110840, relative error: 2.802770e-09</span><br><span class="hljs-comment"># numerical: 1.268882 analytic: 1.268882, relative error: 9.150069e-08</span><br><span class="hljs-comment"># numerical: -2.231693 analytic: -2.231693, relative error: 2.893781e-08</span><br><span class="hljs-comment"># numerical: 1.904050 analytic: 1.904049, relative error: 2.485119e-08</span><br><span class="hljs-comment"># numerical: 1.144526 analytic: 1.144526, relative error: 1.254521e-08</span><br><span class="hljs-comment"># numerical: 1.573668 analytic: 1.573668, relative error: 5.874951e-08</span><br><span class="hljs-comment"># numerical: 0.279931 analytic: 0.279931, relative error: 3.284202e-07</span><br><span class="hljs-comment"># numerical: 2.321664 analytic: 2.321663, relative error: 9.635676e-09</span><br><span class="hljs-comment"># numerical: 4.186737 analytic: 4.186737, relative error: 1.598212e-10</span><br></code></pre></td></tr></table></figure><p><strong>Inline Question 1</strong></p><p>Why do we expect our loss to be close to -log(0.1)? Explain briefly.</p><p><span class="math inline">$\color{blue}{\textit Your Answer:}$</span> <em>When the weights W are initialized to small random values, the logits (scores) computed as X </em> W will be close to zero for all classes. Applying the softmax function to these small logits results in a probability distribution over classes that is approximately uniform. For a 10-class classification task, each class thus has a probability close to 0.1.*</p><p><strong>Inline Question 2</strong></p><p>Although gradcheck is reliable softmax loss, it is possible that for SVM loss, once in a while, a dimension in the gradcheck will not match exactly. What could such a discrepancy be caused by? Is it a reason for concern? What is a simple example in one dimension where a svm loss gradient check could fail? How would change the margin affect of the frequency of this happening?</p><p>Note that SVM loss for a sample <span class="math inline">(<em>x</em><sub><em>i</em></sub>, <em>y</em><sub><em>i</em></sub>)</span> is defined as: <br /><span class="math display"><em>L</em><sub><em>i</em></sub> = ∑<sub><em>j</em> ≠ <em>y</em><sub><em>i</em></sub></sub>max (0, <em>s</em><sub><em>j</em></sub> − <em>s</em><sub><em>y</em><sub><em>i</em></sub></sub> + <em>Δ</em>)</span><br /> where <span class="math inline"><em>j</em></span> iterates over all classes except the correct class <span class="math inline"><em>y</em><sub><em>i</em></sub></span> and <span class="math inline"><em>s</em><sub><em>j</em></sub></span> denotes the classifier score for <span class="math inline"><em>j</em><sup><em>t</em><em>h</em></sup></span> class. <span class="math inline"><em>Δ</em></span> is a scalar margin. For more information, refer to ‘Multiclass Support Vector Machine loss’ on <a href="https://cs231n.github.io/linear-classify/">this</a> page.</p><p><em>Hint: the SVM loss function is not strictly speaking differentiable.</em></p><p><span class="math inline">$\color{blue}{\textit Your Answer:}$</span> <em>For SVM loss, in gradcheck, a dimension may fail to match because the numerical precision or the loss function’s not differentiable. This is not an implementation error but an inherent property of the loss function itself. Example, with a single feature, the gradient at a point is discontinuous, leading to potential inconsistencies between the numerical and analytical gradients. Increasing the margin can reduces the probability of kinks occurring, minimizing mismatches.</em></p><p>向量化计算Softmax损失和梯度，在之前的基础上，类似地，<span class="math inline">$\frac{\partial L_i}{\partial s}$</span> 由 <span class="math inline">$\frac{\partial L_i}{\partial s_j}$</span> 组成，则计算出 <span class="math inline">$\frac{\partial L_i}{\partial s}$</span> 后，可以使用便于向量化计算的公式： <br /><span class="math display">$$\nabla_{W}L_i = \frac{\partial L_i}{\partial s} \frac{\partial s}{\partial W}$$</span><br /> 现在，实现<code>cs231n/classifiers/softmax.py</code>中的<code>softmax_loss_vectorized</code>函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># In cs231n/classifiers/softmax.py</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax_loss_vectorized</span>(<span class="hljs-params">W, X, y, reg</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Softmax loss function, vectorized version.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs and outputs are the same as softmax_loss_naive.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># Initialize the loss and gradient to zero.</span><br>    loss = <span class="hljs-number">0.0</span><br>    dW = np.zeros_like(W)<br><br><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                     #</span><br>    <span class="hljs-comment"># Implement a vectorized version of the softmax loss, storing the           #</span><br>    <span class="hljs-comment"># result in loss.                                                           #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    num_train = X.shape[<span class="hljs-number">0</span>]<br>    scores = X.dot(W)<br>    scores -= np.reshape(np.<span class="hljs-built_in">max</span>(scores, axis=<span class="hljs-number">1</span>), (num_train, <span class="hljs-number">1</span>))<br>    p = np.exp(scores)<br>    p /= np.reshape(np.<span class="hljs-built_in">sum</span>(p, axis=<span class="hljs-number">1</span>), (num_train, <span class="hljs-number">1</span>))<br>    logp = np.log(p)<br>    loss -= np.<span class="hljs-built_in">sum</span>(logp[np.arange(num_train), y])<br>    loss = loss / num_train + reg * np.<span class="hljs-built_in">sum</span>(W * W)<br><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                     #</span><br>    <span class="hljs-comment"># Implement a vectorized version of the gradient for the softmax            #</span><br>    <span class="hljs-comment"># loss, storing the result in dW.                                           #</span><br>    <span class="hljs-comment">#                                                                           #</span><br>    <span class="hljs-comment"># Hint: Instead of computing the gradient from scratch, it may be easier    #</span><br>    <span class="hljs-comment"># to reuse some of the intermediate values that you used to compute the     #</span><br>    <span class="hljs-comment"># loss.                                                                     #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    dscores = p<br>    dscores[np.arange(num_train), y] -= <span class="hljs-number">1</span><br>    dW = X.T.dot(dscores)<br>    dW = dW / num_train + <span class="hljs-number">2</span> * reg * W<br><br>    <span class="hljs-keyword">return</span> loss, dW<br></code></pre></td></tr></table></figure><p>测试损失和梯度的计算，并于之前的循环方法对比运行时间：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Next implement the function softmax_loss_vectorized; for now only compute the loss;</span><br><span class="hljs-comment"># we will implement the gradient in a moment.</span><br>tic = time.time()<br>loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, <span class="hljs-number">0.000005</span>)<br>toc = time.time()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Naive loss: %e computed in %fs&#x27;</span> % (loss_naive, toc - tic))<br><br><span class="hljs-keyword">from</span> cs231n.classifiers.softmax <span class="hljs-keyword">import</span> softmax_loss_vectorized<br>tic = time.time()<br>loss_vectorized, _ = softmax_loss_vectorized(W, X_dev, y_dev, <span class="hljs-number">0.000005</span>)<br>toc = time.time()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Vectorized loss: %e computed in %fs&#x27;</span> % (loss_vectorized, toc - tic))<br><br><span class="hljs-comment"># The losses should match but your vectorized implementation should be much faster.</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;difference: %f&#x27;</span> % (loss_naive - loss_vectorized))<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># Naive loss: 2.344003e+00 computed in 0.058616s</span><br><span class="hljs-comment"># Vectorized loss: 2.344003e+00 computed in 0.010929s</span><br><span class="hljs-comment"># difference: -0.000000</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Complete the implementation of softmax_loss_vectorized, and compute the gradient</span><br><span class="hljs-comment"># of the loss function in a vectorized way.</span><br><br><span class="hljs-comment"># The naive implementation and the vectorized implementation should match, but</span><br><span class="hljs-comment"># the vectorized version should still be much faster.</span><br>tic = time.time()<br>_, grad_naive = softmax_loss_naive(W, X_dev, y_dev, <span class="hljs-number">0.000005</span>)<br>toc = time.time()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Naive loss and gradient: computed in %fs&#x27;</span> % (toc - tic))<br><br>tic = time.time()<br>_, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, <span class="hljs-number">0.000005</span>)<br>toc = time.time()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Vectorized loss and gradient: computed in %fs&#x27;</span> % (toc - tic))<br><br><span class="hljs-comment"># The loss is a single number, so it is easy to compare the values computed</span><br><span class="hljs-comment"># by the two implementations. The gradient on the other hand is a matrix, so</span><br><span class="hljs-comment"># we use the Frobenius norm to compare them.</span><br>difference = np.linalg.norm(grad_naive - grad_vectorized, <span class="hljs-built_in">ord</span>=<span class="hljs-string">&#x27;fro&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;difference: %f&#x27;</span> % difference)<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># Naive loss and gradient: computed in 0.100571s</span><br><span class="hljs-comment"># Vectorized loss and gradient: computed in 0.019927s</span><br><span class="hljs-comment"># difference: 0.000000</span><br></code></pre></td></tr></table></figure><h3 id="stochastic-gradient-descent">Stochastic Gradient Descent</h3><p>使用SGD来减小损失，实现<code>cs231n/classifiers/linear_classifier.py</code>中的<code>train</code>方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># In cs231n/classifiers/linear_classifier.py</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        X,</span><br><span class="hljs-params">        y,</span><br><span class="hljs-params">        learning_rate=<span class="hljs-number">1e-3</span>,</span><br><span class="hljs-params">        reg=<span class="hljs-number">1e-5</span>,</span><br><span class="hljs-params">        num_iters=<span class="hljs-number">100</span>,</span><br><span class="hljs-params">        batch_size=<span class="hljs-number">200</span>,</span><br><span class="hljs-params">        verbose=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Train this linear classifier using stochastic gradient descent.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - X: A numpy array of shape (N, D) containing training data; there are N</span><br><span class="hljs-string">          training samples each of dimension D.</span><br><span class="hljs-string">        - y: A numpy array of shape (N,) containing training labels; y[i] = c</span><br><span class="hljs-string">          means that X[i] has label 0 &lt;= c &lt; C for C classes.</span><br><span class="hljs-string">        - learning_rate: (float) learning rate for optimization.</span><br><span class="hljs-string">        - reg: (float) regularization strength.</span><br><span class="hljs-string">        - num_iters: (integer) number of steps to take when optimizing</span><br><span class="hljs-string">        - batch_size: (integer) number of training examples to use at each step.</span><br><span class="hljs-string">        - verbose: (boolean) If true, print progress during optimization.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Outputs:</span><br><span class="hljs-string">        A list containing the value of the loss function at each training iteration.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        num_train, dim = X.shape<br>        num_classes = (<br>            np.<span class="hljs-built_in">max</span>(y) + <span class="hljs-number">1</span><br>        )  <span class="hljs-comment"># assume y takes values 0...K-1 where K is number of classes</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.W <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># lazily initialize W</span><br>            <span class="hljs-variable language_">self</span>.W = <span class="hljs-number">0.001</span> * np.random.randn(dim, num_classes)<br><br>        <span class="hljs-comment"># Run stochastic gradient descent to optimize W</span><br>        loss_history = []<br>        <span class="hljs-keyword">for</span> it <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iters):<br>            X_batch = <span class="hljs-literal">None</span><br>            y_batch = <span class="hljs-literal">None</span><br><br>            <span class="hljs-comment">#########################################################################</span><br>            <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                 #</span><br>            <span class="hljs-comment"># Sample batch_size elements from the training data and their           #</span><br>            <span class="hljs-comment"># corresponding labels to use in this round of gradient descent.        #</span><br>            <span class="hljs-comment"># Store the data in X_batch and their corresponding labels in           #</span><br>            <span class="hljs-comment"># y_batch; after sampling X_batch should have shape (batch_size, dim)   #</span><br>            <span class="hljs-comment"># and y_batch should have shape (batch_size,)                           #</span><br>            <span class="hljs-comment">#                                                                       #</span><br>            <span class="hljs-comment"># Hint: Use np.random.choice to generate indices. Sampling with         #</span><br>            <span class="hljs-comment"># replacement is faster than sampling without replacement.              #</span><br>            <span class="hljs-comment">#########################################################################</span><br>            indices = np.random.choice(num_train, size=batch_size, replace=<span class="hljs-literal">True</span>)<br>            X_batch = X[indices]<br>            y_batch = y[indices]<br><br>            <span class="hljs-comment"># evaluate loss and gradient</span><br>            loss, grad = <span class="hljs-variable language_">self</span>.loss(X_batch, y_batch, reg)<br>            loss_history.append(loss)<br><br>            <span class="hljs-comment"># perform parameter update</span><br>            <span class="hljs-comment">#########################################################################</span><br>            <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                 #</span><br>            <span class="hljs-comment"># Update the weights using the gradient and the learning rate.          #</span><br>            <span class="hljs-comment">#########################################################################</span><br>            <span class="hljs-variable language_">self</span>.W -= learning_rate * grad<br><br>            <span class="hljs-keyword">if</span> verbose <span class="hljs-keyword">and</span> it % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;iteration %d / %d: loss %f&quot;</span> % (it, num_iters, loss))<br><br>        <span class="hljs-keyword">return</span> loss_history<br></code></pre></td></tr></table></figure><p>训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># In the file linear_classifier.py, implement SGD in the function</span><br><span class="hljs-comment"># LinearClassifier.train() and then run it with the code below.</span><br><span class="hljs-keyword">from</span> cs231n.classifiers <span class="hljs-keyword">import</span> Softmax<br>softmax = Softmax()<br>tic = time.time()<br>loss_hist = softmax.train(X_train, y_train, learning_rate=<span class="hljs-number">1e-7</span>, reg=<span class="hljs-number">2.5e4</span>,<br>                      num_iters=<span class="hljs-number">1500</span>, verbose=<span class="hljs-literal">True</span>)<br>toc = time.time()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;That took %fs&#x27;</span> % (toc - tic))<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># iteration 0 / 1500: loss 784.261531</span><br><span class="hljs-comment"># iteration 100 / 1500: loss 287.556184</span><br><span class="hljs-comment"># iteration 200 / 1500: loss 106.569326</span><br><span class="hljs-comment"># iteration 300 / 1500: loss 40.286102</span><br><span class="hljs-comment"># iteration 400 / 1500: loss 15.996293</span><br><span class="hljs-comment"># iteration 500 / 1500: loss 7.167806</span><br><span class="hljs-comment"># iteration 600 / 1500: loss 3.924627</span><br><span class="hljs-comment"># iteration 700 / 1500: loss 2.739457</span><br><span class="hljs-comment"># iteration 800 / 1500: loss 2.365508</span><br><span class="hljs-comment"># iteration 900 / 1500: loss 2.133507</span><br><span class="hljs-comment"># iteration 1000 / 1500: loss 2.143132</span><br><span class="hljs-comment"># iteration 1100 / 1500: loss 2.142237</span><br><span class="hljs-comment"># iteration 1200 / 1500: loss 2.054092</span><br><span class="hljs-comment"># iteration 1300 / 1500: loss 2.097079</span><br><span class="hljs-comment"># iteration 1400 / 1500: loss 2.074805</span><br><span class="hljs-comment"># That took 11.690820s</span><br></code></pre></td></tr></table></figure><p>可视化结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># A useful debugging strategy is to plot the loss as a function of</span><br><span class="hljs-comment"># iteration number:</span><br>plt.plot(loss_hist)<br>plt.xlabel(<span class="hljs-string">&#x27;Iteration number&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss value&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><figure><img src="/images/cs231n/loss-history.png" alt="the loss as a function of iteration number" /><figcaption>the loss as a function of iteration number</figcaption></figure><p>实现<code>cs231n/classifiers/linear_classifier.py</code>中的<code>predict</code>方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># In cs231n/classifiers/linear_classifier.py</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Use the trained weights of this linear classifier to predict labels for</span><br><span class="hljs-string">        data points.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - X: A numpy array of shape (N, D) containing training data; there are N</span><br><span class="hljs-string">          training samples each of dimension D.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional</span><br><span class="hljs-string">          array of length N, and each element is an integer giving the predicted</span><br><span class="hljs-string">          class.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        y_pred = np.zeros(X.shape[<span class="hljs-number">0</span>])<br>        <span class="hljs-comment">###########################################################################</span><br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                   #</span><br>        <span class="hljs-comment"># Implement this method. Store the predicted labels in y_pred.            #</span><br>        <span class="hljs-comment">###########################################################################</span><br>        scores = X.dot(<span class="hljs-variable language_">self</span>.W)<br>        y_pred = np.argmax(scores,axis=<span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">return</span> y_pred<br></code></pre></td></tr></table></figure><p>使用模型预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Write the LinearClassifier.predict function and evaluate the performance on</span><br><span class="hljs-comment"># both the training and validation set</span><br><span class="hljs-comment"># You should get validation accuracy of about 0.34 (&gt; 0.33).</span><br>y_train_pred = softmax.predict(X_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;training accuracy: %f&#x27;</span> % (np.mean(y_train == y_train_pred), ))<br>y_val_pred = softmax.predict(X_val)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;validation accuracy: %f&#x27;</span> % (np.mean(y_val == y_val_pred), ))<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># training accuracy: 0.328122</span><br><span class="hljs-comment"># validation accuracy: 0.343000</span><br></code></pre></td></tr></table></figure><p>保存模型（具体代码在cs231n/classifiers/linear_classifier.py中，这里实际上是保存W矩阵）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Save the trained model for autograder.</span><br>softmax.save(<span class="hljs-string">&quot;softmax.npy&quot;</span>)<br></code></pre></td></tr></table></figure><p>交叉验证调整超参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Use the validation set to tune hyperparameters (regularization strength and</span><br><span class="hljs-comment"># learning rate). You should experiment with different ranges for the learning</span><br><span class="hljs-comment"># rates and regularization strengths; if you are careful you should be able to</span><br><span class="hljs-comment"># get a classification accuracy of about 0.365 (&gt; 0.36) on the validation set.</span><br><br><span class="hljs-comment"># Note: you may see runtime/overflow warnings during hyper-parameter search.</span><br><span class="hljs-comment"># This may be caused by extreme values, and is not a bug.</span><br><br><span class="hljs-comment"># results is dictionary mapping tuples of the form</span><br><span class="hljs-comment"># (learning_rate, regularization_strength) to tuples of the form</span><br><span class="hljs-comment"># (training_accuracy, validation_accuracy). The accuracy is simply the fraction</span><br><span class="hljs-comment"># of data points that are correctly classified.</span><br>results = &#123;&#125;<br>best_val = -<span class="hljs-number">1</span>   <span class="hljs-comment"># The highest validation accuracy that we have seen so far.</span><br>best_softmax = <span class="hljs-literal">None</span> <span class="hljs-comment"># The Softmax object that achieved the highest validation rate.</span><br><br><span class="hljs-comment">################################################################################</span><br><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>                                                                        #</span><br><span class="hljs-comment"># Write code that chooses the best hyperparameters by tuning on the validation #</span><br><span class="hljs-comment"># set. For each combination of hyperparameters, train a Softmax on the.        #</span><br><span class="hljs-comment"># training set, compute its accuracy on the training and validation sets, and  #</span><br><span class="hljs-comment"># store these numbers in the results dictionary. In addition, store the best   #</span><br><span class="hljs-comment"># validation accuracy in best_val and the Softmax object that achieves this.   #</span><br><span class="hljs-comment"># accuracy in best_softmax.                                                    #</span><br><span class="hljs-comment">#                                                                              #</span><br><span class="hljs-comment"># Hint: You should use a small value for num_iters as you develop your         #</span><br><span class="hljs-comment"># validation code so that the classifiers don&#x27;t take much time to train; once  #</span><br><span class="hljs-comment"># you are confident that your validation code works, you should rerun the      #</span><br><span class="hljs-comment"># code with a larger value for num_iters.                                      #</span><br><span class="hljs-comment">################################################################################</span><br><br><span class="hljs-comment"># Provided as a reference. You may or may not want to change these hyperparameters</span><br>learning_rates = [<span class="hljs-number">1e-7</span>, <span class="hljs-number">1e-6</span>]<br>regularization_strengths = [<span class="hljs-number">2.5e4</span>, <span class="hljs-number">1e4</span>]<br><br><span class="hljs-keyword">for</span> lr <span class="hljs-keyword">in</span> learning_rates:<br>  <span class="hljs-keyword">for</span> reg <span class="hljs-keyword">in</span> regularization_strengths:<br>    soft = Softmax()<br>    softmax.train(X_train, y_train, learning_rate=lr, reg=reg, num_iters=<span class="hljs-number">1500</span>)<br>    y_train_pred = softmax.predict(X_train)<br>    y_val_pred = softmax.predict(X_val)<br>    training_accuracy = np.mean(y_train == y_train_pred)<br>    validation_accuracy = np.mean(y_val == y_val_pred)<br>    results[(lr, reg)] = (training_accuracy, validation_accuracy)<br>    <span class="hljs-keyword">if</span> validation_accuracy &gt; best_val:<br>      best_val = validation_accuracy<br>      best_softmax = softmax<br><br><span class="hljs-comment"># Print out results.</span><br><span class="hljs-keyword">for</span> lr, reg <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(results):<br>    train_accuracy, val_accuracy = results[(lr, reg)]<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;lr %e reg %e train accuracy: %f val accuracy: %f&#x27;</span> % (<br>                lr, reg, train_accuracy, val_accuracy))<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;best validation accuracy achieved during cross-validation: %f&#x27;</span> % best_val)<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.356714 val accuracy: 0.371000</span><br><span class="hljs-comment"># lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.326122 val accuracy: 0.342000</span><br><span class="hljs-comment"># lr 1.000000e-06 reg 1.000000e+04 train accuracy: 0.350102 val accuracy: 0.355000</span><br><span class="hljs-comment"># lr 1.000000e-06 reg 2.500000e+04 train accuracy: 0.321857 val accuracy: 0.334000</span><br><span class="hljs-comment"># best validation accuracy achieved during cross-validation: 0.371000</span><br></code></pre></td></tr></table></figure><p>结果可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Visualize the cross-validation results</span><br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> pdb<br><br><span class="hljs-comment"># pdb.set_trace()</span><br><br>x_scatter = [math.log10(x[<span class="hljs-number">0</span>]) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> results]<br>y_scatter = [math.log10(x[<span class="hljs-number">1</span>]) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> results]<br><br><span class="hljs-comment"># plot training accuracy</span><br>marker_size = <span class="hljs-number">100</span><br>colors = [results[x][<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> results]<br>plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>plt.tight_layout(pad=<span class="hljs-number">3</span>)<br>plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap=plt.cm.coolwarm)<br>plt.colorbar()<br>plt.xlabel(<span class="hljs-string">&#x27;log learning rate&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;log regularization strength&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;CIFAR-10 training accuracy&#x27;</span>)<br><br><span class="hljs-comment"># plot validation accuracy</span><br>colors = [results[x][<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> results] <span class="hljs-comment"># default size of markers is 20</span><br>plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap=plt.cm.coolwarm)<br>plt.colorbar()<br>plt.xlabel(<span class="hljs-string">&#x27;log learning rate&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;log regularization strength&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;CIFAR-10 validation accuracy&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>使用得到的表现最好的模型进行测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Evaluate the best softmax on test set</span><br>y_test_pred = best_softmax.predict(X_test)<br>test_accuracy = np.mean(y_test == y_test_pred)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Softmax classifier on raw pixels final test set accuracy: %f&#x27;</span> % test_accuracy)<br><br><span class="hljs-comment"># Output:</span><br><span class="hljs-comment"># Softmax classifier on raw pixels final test set accuracy: 0.359000</span><br></code></pre></td></tr></table></figure><p>保存模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Save best softmax model</span><br>best_softmax.save(<span class="hljs-string">&quot;best_softmax.npy&quot;</span>)<br></code></pre></td></tr></table></figure><p>可视化模型学习到的各个类别的权重：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Visualize the learned weights for each class.</span><br><span class="hljs-comment"># Depending on your choice of learning rate and regularization strength, these may</span><br><span class="hljs-comment"># or may not be nice to look at.</span><br>w = best_softmax.W[:-<span class="hljs-number">1</span>,:] <span class="hljs-comment"># strip out the bias</span><br>w = w.reshape(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>)<br>w_min, w_max = np.<span class="hljs-built_in">min</span>(w), np.<span class="hljs-built_in">max</span>(w)<br>classes = [<span class="hljs-string">&#x27;plane&#x27;</span>, <span class="hljs-string">&#x27;car&#x27;</span>, <span class="hljs-string">&#x27;bird&#x27;</span>, <span class="hljs-string">&#x27;cat&#x27;</span>, <span class="hljs-string">&#x27;deer&#x27;</span>, <span class="hljs-string">&#x27;dog&#x27;</span>, <span class="hljs-string">&#x27;frog&#x27;</span>, <span class="hljs-string">&#x27;horse&#x27;</span>, <span class="hljs-string">&#x27;ship&#x27;</span>, <span class="hljs-string">&#x27;truck&#x27;</span>]<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, i + <span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># Rescale the weights to be between 0 and 255</span><br>    wimg = <span class="hljs-number">255.0</span> * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)<br>    plt.imshow(wimg.astype(<span class="hljs-string">&#x27;uint8&#x27;</span>))<br>    plt.axis(<span class="hljs-string">&#x27;off&#x27;</span>)<br>    plt.title(classes[i])<br></code></pre></td></tr></table></figure><figure><img src="/images/cs231n/softmax-weights.png" alt="Visualize the learned weights for each class" /><figcaption>Visualize the learned weights for each class</figcaption></figure><p><strong>Inline question 3</strong></p><p>Describe what your visualized Softmax classifier weights look like, and offer a brief explanation for why they look the way they do.</p><p><span class="math inline">$\color{blue}{\textit Your Answer:}$</span> <em>The visualized Softmax classifier weights typically show blurred, low-contrast “templates” that roughly correspond to the key visual features of each class. For example, the weights of class car look like a body of car and its windows. Because the Softmax classifier learns linear decision boundaries. The weights act as feature detectors: during training, each class’s weights adjust to amplify pixel values that are statistically characteristic of that class and suppress irrelevant ones. The blurriness results from the model averaging over diverse training examples, capturing commonalities rather than sharp details. Regularization also smooths the weights, preventing overfitting to noise in the data.</em></p><p><strong>Inline Question 4</strong> - <em>True or False</em></p><p>Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would change the softmax loss, but leave the SVM loss unchanged.</p><p><span class="math inline">$\color{blue}{\textit Your Answer:}$</span> True</p><p><span class="math inline">$\color{blue}{\textit Your Explanation:}$</span> For the SVM loss, a new datapoint leaves the loss unchanged if, for its true class <span class="math inline"><em>y</em><sub><em>i</em></sub></span>, all other classes <span class="math inline"><em>j</em> ≠ <em>y</em><sub><em>i</em></sub></span> satisfy <span class="math inline"><em>s</em><sub><em>j</em></sub> − <em>s</em><sub><em>y</em><sub><em>i</em></sub></sub> + <em>Δ</em> ≤ 0</span> (i.e., no margins are violated). In this case, the max(0, ·) term for all <span class="math inline"><em>j</em> ≠ <em>y</em><sub><em>i</em></sub></span> is 0, so the per-datapoint SVM loss is 0, and adding it does not change the total loss. For the softmax loss, however, the loss for a datapoint depends on the probability of its true class (<span class="math inline"> − <em>l</em><em>o</em><em>g</em>(<em>p</em><sub><em>y</em><sub><em>i</em></sub></sub>)</span>), which is never exactly 0 (since softmax probabilities are always positive).</p><h2 id="q3-two-layer-neural-network">Q3: Two-Layer Neural Network</h2><p>这部分主要关注<code>two_layer_net.ipynb</code>，将实现一个两层的神经网络。</p><blockquote><p>从这里开始，将减少一些不是很重要的细节，相信能看到这里，一些细节不需要详细的描述了。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;由于毕设需要，且没怎么学习过AI这块，所以浅浅记录一下，但不会像之前记录MIT6.5840那样在实现细节上一笔带过。原因的话，对于分布式这块，好歹我有过一点分布式系统的经验以及一些知识，但AI这块，我仅仅之前半途而废地上过一点网课…&lt;/p&gt;
&lt;/b</summary>
      
    
    
    
    <category term="计算机" scheme="https://logicff.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    
    <category term="计算机视觉" scheme="https://logicff.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="AI" scheme="https://logicff.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>6.5840：Lab2 - Key/Value Server</title>
    <link href="https://logicff.github.io/2025/11/09/6.5840-2/"/>
    <id>https://logicff.github.io/2025/11/09/6.5840-2/</id>
    <published>2025-11-09T07:35:00.000Z</published>
    <updated>2025-11-09T07:35:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Key-Value-Server实验"><a href="#Key-Value-Server实验" class="headerlink" title="Key&#x2F;Value Server实验"></a>Key&#x2F;Value Server实验</h2><p>这个实验感觉还是很简单的，上个月（2025-10）月底看完vm-ft论文后没花多久就做完了。感觉与之前做完的Lab1和最近做完的Lab3A相比，这个实验是为了熟悉一下K&#x2F;V存储，对RPC和分布式系统有一个更加深刻的认知，为后面的实验做铺垫，而且个人认为，即使不看vm-ft论文也能做这个实验，具体实验任务和提示都比较详细，除了基于K&#x2F;V服务的分布式锁外，其他任务基本上是“照着做”就行了。</p><p>这里呈上我自己的实现<a href="https://github.com/logicff/mit6.5840">https://github.com/logicff/mit6.5840</a>。</p><h2 id="Fault-Tolerant-Virtual-Machines（vm-ft论文）"><a href="#Fault-Tolerant-Virtual-Machines（vm-ft论文）" class="headerlink" title="Fault-Tolerant Virtual Machines（vm-ft论文）"></a>Fault-Tolerant Virtual Machines（vm-ft论文）</h2><p>理解论文其实不难，主要的点就是VMware FT的主从备份架构和FT Protocol，以及使用replicated state-machines，很多关于如何保证可用性和一致性的细节都有在论文中提到过。个人建议的话，可以先读论文（即使是粗略的读），然后看一下课程，最后再看看6.5840的关于vm-ft的FAQ。</p><h2 id="一点想法"><a href="#一点想法" class="headerlink" title="一点想法"></a>一点想法</h2><ol><li><p>关于FT Protocol的一个可能优化：按照论文中的描述，一个带有输出的请求的大致流程应该是<code>请求发送到主虚拟机-&gt;主虚拟机的VMM向备份虚拟机发送日志条目并让主虚拟机处理请求-&gt;主虚拟机的VMM收到备份虚拟机的确认后转发主虚拟机的输出</code>，如果我们让输出从备份虚拟机发出呢，这样是不是相当于在处理时间相远小于虚拟机之间传输时间的情况下，可以提升整个系统的效率，为了保证一定的正确，备份虚拟机在发出输出的同时向主虚拟机确认即可，这种方案在外部看来从输入到输出相比原方案快了一个主备单向交互时间。不过，我暂时无法验证具体可行性，毕竟对目前的我而言还是很难。</p></li><li><p>面对multi-processor，VMware FT的主备虚拟机无法保证同步，相同的程序在不同的计算机上可能会有不同的指令流。这种情况的一种解决方案是采用内存快照进行同步，但一次内存快照的传输时间可能会很大，我的一个想法是局部快照（比如，只对有变化的内存进行快照，好像已经有个说法叫增量快照了），感觉效率上应该能提升较大。</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Key-Value-Server实验&quot;&gt;&lt;a href=&quot;#Key-Value-Server实验&quot; class=&quot;headerlink&quot; title=&quot;Key&amp;#x2F;Value Server实验&quot;&gt;&lt;/a&gt;Key&amp;#x2F;Value Server实验&lt;/h2</summary>
      
    
    
    
    <category term="计算机" scheme="https://logicff.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    
    <category term="分布式系统" scheme="https://logicff.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>6.5840：Lab1 - MapReduce</title>
    <link href="https://logicff.github.io/2025/10/12/6.5840-1/"/>
    <id>https://logicff.github.io/2025/10/12/6.5840-1/</id>
    <published>2025-10-12T12:08:36.000Z</published>
    <updated>2025-10-12T12:08:36.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/6.5840/mapreduce.png" alt="MapReduce Overview"></p><blockquote><p>这里直接引用论文中的图片，本文会忽略一些东西，主要记录一下我自己的理解。</p></blockquote><h2 id="分布式系统的特性"><a href="#分布式系统的特性" class="headerlink" title="分布式系统的特性"></a>分布式系统的特性</h2><ol><li><p>可扩展性。通俗来讲就是如果我用一台计算机解决了一些问题，当我增加第二台计算机后，我只需要一半的时间就可以解决这些问题，或者说每分钟可以解决两倍数量的问题。这在MapReduce中体现为将一个大任务分解成很多的小任务，以便于多个worker同时处理任务。</p></li><li><p>可用性。经过精心的设计，在特定的错误类型下（如某台机器发生故障），系统仍然能够正常运行，仍然可以像没有出现错误一样，提供完整的服务。一般的方法如构建一个多副本系统，其中一个故障了，其他的还能运行，当然如果所有副本都故障了，系统就不再有可用性。这在MapReduce中体现为多个worker可以互为副本，其中一个worker发生故障，可以将其任务交给另一个worker来执行。</p></li><li><p>可恢复性。如果出现了问题，服务会停止工作，不再响应请求，之后有人来修复，并且在修复之后系统仍然可以正常运行，就像没有出现过问题一样，这是一个比可用性更弱的需求。实现可用性和可恢复性这两个特性，有很多方法，其中最重要的两个是非易失存储和副本。</p></li><li><p>一致性。包括多个副本之间要保持一致，不过强一致性的代价非常高，人们可能对弱一致性感兴趣。</p></li></ol><h2 id="MapReduce实验"><a href="#MapReduce实验" class="headerlink" title="MapReduce实验"></a>MapReduce实验</h2><p>与论文所描述的有些不同，实验推荐采用worker向Master请求任务，当然，这是一种简化的做法，实现起来相对比较容易。但有一点，这种情况下，Master处于一个较为被动的情景。其他方面与论文描述的差不多。</p><p>在我看来，实验的有一点设计的比较巧妙，那就是为了保证未完全完成的任务结果影响到其他正常任务，采用创建临时文件保存未完全完成的任务结果，待任务完全完成后进行文件改名的原子性操作。</p><p>很多细节我建议阅读论文以及具体实验任务和提示，这里呈上我自己的实现<a href="https://github.com/logicff/mit6.5840">https://github.com/logicff/mit6.5840</a>。</p><h2 id="一点想法"><a href="#一点想法" class="headerlink" title="一点想法"></a>一点想法</h2><ol><li><p>关于如何按照论文所描述的那样让Master主动分配任务。我的想法是让能够处理任务的worker向Master暴露自己，然后Master将已暴露的worker相关信息保存起来，之后分配任务的时候就可以根据保存的信息进行调度，还有超时检测，相比实验中设置一个超时时间，我认为更加合理的是在已有worker信息的基础上采用心跳检测等机制。</p></li><li><p>关于如何在多台机器上运行实验。其实实验任务中已经给了提示，就是用TCP&#x2F;IP代替Unix sockets。</p></li><li><p>关于进一步提升MapReduce的性能。其实之前的课堂视频上有人提到过采用流式（或者说流水线）的方法进一步提高效率，而不是让Reduce worker等待Map worker全部完成任务。</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/images/6.5840/mapreduce.png&quot; alt=&quot;MapReduce Overview&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这里直接引用论文中的图片，本文会忽略一些东西，主要记录一下我自己的理解。&lt;/p&gt;
&lt;/blockquo</summary>
      
    
    
    
    <category term="计算机" scheme="https://logicff.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    
    <category term="分布式系统" scheme="https://logicff.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>旧物：回忆是痛苦还是美好？</title>
    <link href="https://logicff.github.io/2025/07/26/oldthings/"/>
    <id>https://logicff.github.io/2025/07/26/oldthings/</id>
    <published>2025-07-26T06:29:30.000Z</published>
    <updated>2025-07-26T06:29:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天准备收拾东西回家，顺便对我那堆杂乱的东西进行一波小整理，于是翻出来了一些旧物，如图：</p><p><img src="/images/oldthings/oldthings1.jpg" alt="一些旧物"></p><p>电子钟是自己亲手焊的，只记得当时空气中弥漫着焊丝（好像是锡丝吧）的味道，真的不想再闻一次。</p><p>塑料冰墩墩是3D打印的，由于这个相当于是一个测试结果，当时老师没有收，就被我拿走了，不过熔融状态的塑料散发出的味道我是不想再闻了。</p><p>圆柱状的东西好像是用来做小零件的材料，忘了是由于什么原因给带走了。</p><p>鸭嘴锤也是自己亲手做的，是做钳工的结果，回想起来好像除了累还是累。</p><p>现在看到这几个东西总有种说不清的感觉，也许是怀念，也许是痛苦，也许只是回忆涌上心头，夹杂着一些惆怅。</p><p>最后这块机械表是高中的时候我妈给我买的，我以前都有看时间的习惯，所以基本上手上都会戴着表，这个习惯对我蛮有影响的，即使现在不戴表了也会时不时看一下时间。这块表好像是大一的时候表带断了，但我没去买新的，就把这块表放口袋里，想看时间的时候就掏出来看一下。后面不知道什么时候坏了，今天翻出来的时候还是有种不一样的感觉的，我想，应该是美好的吧。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;今天准备收拾东西回家，顺便对我那堆杂乱的东西进行一波小整理，于是翻出来了一些旧物，如图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/oldthings/oldthings1.jpg&quot; alt=&quot;一些旧物&quot;&gt;&lt;/p&gt;
&lt;p&gt;电子钟是自己亲手焊的，只记得当时空气中弥漫着</summary>
      
    
    
    
    <category term="随笔" scheme="https://logicff.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="日常" scheme="https://logicff.github.io/tags/%E6%97%A5%E5%B8%B8/"/>
    
    <category term="回忆" scheme="https://logicff.github.io/tags/%E5%9B%9E%E5%BF%86/"/>
    
  </entry>
  
  <entry>
    <title>日常：浅浅装一下</title>
    <link href="https://logicff.github.io/2025/07/01/daily-2025-07-01/"/>
    <id>https://logicff.github.io/2025/07/01/daily-2025-07-01/</id>
    <published>2025-07-01T13:49:28.000Z</published>
    <updated>2025-07-01T13:49:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>事情是这样的，学校有一门极其抽象的特色课程，大家对其的评价都是…</p><p>咳咳，总之难评就是了，我也可以说是被此课程折磨了一学期🤣</p><p>不过，今天这门课的成绩出来了，将近满分，也不枉我被折磨了这么久，转念一想，既然大家都被折磨了（不然这门课也不会“好评如潮”），而我的分数看起来也蛮不错，因此，必须浅浅装一下：</p><p>😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎</p><p>（作者精神状态良好）</p><p>言归正传，如果自己以后看到这篇随笔的话，可能会觉得有点无语，不过好歹也算是大学期间的一件令我印象深刻的事情。不过感觉这学期能让我印象深刻的事似乎挺多的，除了这门抽象课外，还有一些积极的，像钢琴、数据库、编译器、SDN等等，不知道以后还记不记得。</p>]]></content>
    
    
    <summary type="html">😎😎😎</summary>
    
    
    
    <category term="随笔" scheme="https://logicff.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="日常" scheme="https://logicff.github.io/tags/%E6%97%A5%E5%B8%B8/"/>
    
  </entry>
  
  <entry>
    <title>吐槽：流氓软件随意读取相册</title>
    <link href="https://logicff.github.io/2025/06/26/android-gallery-permission-cn/"/>
    <id>https://logicff.github.io/2025/06/26/android-gallery-permission-cn/</id>
    <published>2025-06-26T09:19:22.000Z</published>
    <updated>2025-06-26T09:19:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>下午在网上surfing时，遇到一条帖子，说是国内某“笔记”软件读取了他&#x2F;她&#x2F;它的手机相册，读取照片四百多次（几乎五百次），不由得一惊，我立马跑去看手机的隐私访问记录（本人安卓手机），然后发现国内某音乐软件读取了我的相册，如图：</p><p><img src="/images/gallery/android-gallery-permission/permission-record1.png" alt="国内某音乐软件读取相册记录1"></p><p><img src="/images/gallery/android-gallery-permission/permission-record2.png" alt="国内某音乐软件读取相册记录2"></p><p>由于系统仅记录近7天的隐私访问记录，遂一时间无法立刻揪出我手机中的所有流氓软件，不过令我震惊的是，此国内某音乐软件每次打开时都会把我的相册扫描一遍，离谱的是，我的相册里总共24个视频全被读取了，总共627张图片被读取了511张。</p><p>至于为什么图片没有被全部读取，应该是因为我的一些图片放在了比较“偏僻”的路径下。好巧不巧，刷到一个知乎回答，答主针对安卓手机做了两个测试App，其中一个是仅允许读取选中的图片，另一个是全程不需要用户参与就自动读取相册中的图片，第一个的结果就不说了，第二个的结果是，答主手机上的7445张照片被读取了4587张，没有全部读取的原因应该和我的一样。</p><p>在这个方面上，国外就做的很不错了，我所用过的国外软件（ins、纸飞机、国内软件的海外版等等）都非常尊重用户的隐私，拿读取照片来说，一般国外软件都会完整地给出“使用选中的照片和视频”、“允许使用全部的照片和视频”、“禁止”这三个选项；而国内软件一般只给两个选项，要么允许要么拒绝，你允许就当你允许全部，你拒绝就“服务不可用”，简直就是流氓。</p><p>话不多说，马上手动设置（没错，还得手动设置，不知道的用户只能不知不觉地承担风险了）一些软件的权限为“使用选中的照片和视频”，还有一些软件由于不支持只能暂时被迫“允许使用全部的照片和视频”。</p><p>我听说，具有私自上传窃听用户隐私的App是没法上架Apple的App Store的，如果这是真的，那么又是羡慕Apple的一天。（还有Google Play）😭</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;下午在网上surfing时，遇到一条帖子，说是国内某“笔记”软件读取了他&amp;#x2F;她&amp;#x2F;它的手机相册，读取照片四百多次（几乎五百次），不由得一惊，我立马跑去看手机的隐私访问记录（本人安卓手机），然后发现国内某音乐软件读取了我的相册，如图：&lt;/p&gt;
&lt;p&gt;&lt;img </summary>
      
    
    
    
    <category term="随笔" scheme="https://logicff.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="相册" scheme="https://logicff.github.io/tags/%E7%9B%B8%E5%86%8C/"/>
    
    <category term="安全" scheme="https://logicff.github.io/tags/%E5%AE%89%E5%85%A8/"/>
    
    <category term="隐私" scheme="https://logicff.github.io/tags/%E9%9A%90%E7%A7%81/"/>
    
  </entry>
  
  <entry>
    <title>Git：分布式版本控制系统</title>
    <link href="https://logicff.github.io/2025/06/24/git/"/>
    <id>https://logicff.github.io/2025/06/24/git/</id>
    <published>2025-06-24T06:57:31.000Z</published>
    <updated>2025-07-01T14:25:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>考完了这学期的期末考试，算是有了一小段空闲时间（大概一周左右），突然想到之前的Git学习还没结束，学习笔记也停了好久，遂浅浅写一篇随笔并继续我的Git学习。</p><h2 id="一、Git是什么？"><a href="#一、Git是什么？" class="headerlink" title="一、Git是什么？"></a>一、Git是什么？</h2><p>Git是一个开源的分布式版本控制系统，可以有效、高速地处理从很小到非常大的项目版本管理。Linus Torvalds（Linux内核的发明者）称其为“The stupid content tracker”，傻瓜式内容跟踪器。</p><p>对于版本控制，Git采用记录文件的改动的方式来记录版本，而不是每个版本都保存一份完整的备份。例如，在文件的第5行加了一个单词“Linux”，在第8行删了一个单词“Windows”，Git就会把这些改动记录下来。</p><p>不过目前为止，暂时只能跟踪文本文件的改动，比如TXT文件，网页文件，所有的程序代码等等。不幸的是，Word文档的格式是二进制格式，因此，版本控制系统是没法跟踪Word文件的改动的。如果你实在想要利用Git来管理你的Word论文，也不是没有办法，我的一个想法是，可以先用文本文件写论文，图片暂时用文字替代一下，等写完后再全部放到Word中处理格式，听起来有点像写Markdown啊…</p><p>值得学习的是，这种利用局部改动特性的思想。如果你需要写长篇大论的文章，有时候想删除一个段落继续写，又怕将来想恢复找不回来，你会怎么办，或者说怎么去备份呢？一般人可能会这么做，先把当前文件“另存为……”一个文件，再接着改，改到一定程度，再“另存为……”一个文件，这样下去不仅浪费了大量的存储空间，而且文件数量越来越多，难以管理。</p><p>这种思想其实在很多领域都有影子，作为一个CSer，我所了解的就有版本控制、增量备份&#x2F;快照、日志记录、数据库变更等等。</p><p>对于分布式，与集中式相对，Git的分布式特性使得开发者间的协作变得更加灵活多样。我就不详细介绍了，<del>关于分布式我只想吐槽为什么不把“distributed”翻译的更浅显易懂一些，叫“分布式”真是让初学者摸不着头脑，什么听起来高大上的玩意，搞的我当初接触这个概念的时候以为是什么高深技术，一般人学不会的那种，现在看来真是误人子弟</del>。</p><h2 id="二、学习Git"><a href="#二、学习Git" class="headerlink" title="二、学习Git"></a>二、学习Git</h2><p>我就不介绍各种命令该怎么用了，相比官方文档，我只有献丑之技。分享一下目前（2025-06-24）为止我个人的学习经验和避坑指南：</p><p>推荐去<a href="https://git-scm.com/">官方网站</a>学习，在<a href="https://git-scm.com/doc">官方文档Documentation</a>中，不仅可以通过man pages学习具体命令（大部分命令点进去后可以通过右上角区域选择中文，对英语不太好的朋友非常友好），而且可以免费获取官方的书，叫Pro Git，对Git有一个非常详细的介绍，这里奉上中文版链接<a href="https://git-scm.com/book/zh/v2">https://git-scm.com/book/zh/v2</a>，可以下载下来慢慢阅读。</p><p>如果你需要的是快速入门，那么你可以去看看廖雪峰的<a href="https://liaoxuefeng.com/books/git/introduction/index.html">Git教程</a>。</p><p>我不太推荐去CSDN学习Git，具体原因就不展开说了，<del>毕竟CSDN的吃相太难看了，风气也难评，学点琐碎的勉强还行，Git学习还是别在CSDN上浪费时间了（相比上面的学习路径）</del>，有官方文档或者优质教程的尽量去看官方文档和优质教程。</p><h2 id="三、一些Git学习记录"><a href="#三、一些Git学习记录" class="headerlink" title="三、一些Git学习记录"></a>三、一些Git学习记录</h2><ul><li><p>2024-07-10：下载Git，开始使用git clone命令</p></li><li><p>2025-04-05：发现除了git clone命令外就不会Git了，于是开始学习Git</p></li><li><p>2025-04-07：到目前（2025-04-07）为止已经学习完了(git) config, init, clone, add, commit, status, diff, log等命令，还有工作目录、暂存区域以及Git仓库的概念，但由于其他事情暂停Git的学习</p></li><li><p>2025-06-24：继续Git的学习，又学习了(git) rm, mv, reset, checkout, reflog等命令</p></li><li><p>2025-06-25：学习了HEAD、Index和Working Directory这三个区域的概念，以及(git) reset, checkout是如何操纵这三个区域的</p></li><li><p>2025-06-26：了解了(git) remote, fetch, pull, push，想在Github上建个仓库继续学习Git；与此同时，突然想起之前做SDN实验时，用vscode就能简单地“图形化”Git，当我看到Ryu的Git仓库图形时，开始思考用这种“图形化界面”会不会更好呢，毕竟花更少的时间就能得到不错的效益</p></li><li><p>2025-06-27：Git远程仓库基础实操</p></li><li><p>2025-06-28：学习Git标签，分支创建、合并、管理</p></li><li><p>2025-06-29：大概学了一下解决分支合并冲突，但由于其他事情暂停Git的学习，不过Git学的应该差不多了，暂停Git的学习不代表暂停Git的使用</p></li></ul>]]></content>
    
    
    <summary type="html">Git是目前世界上最先进的分布式版本控制系统。</summary>
    
    
    
    <category term="工具" scheme="https://logicff.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="Git" scheme="https://logicff.github.io/tags/Git/"/>
    
    <category term="版本控制" scheme="https://logicff.github.io/tags/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/"/>
    
  </entry>
  
  <entry>
    <title>本站指引</title>
    <link href="https://logicff.github.io/2025/06/22/top/"/>
    <id>https://logicff.github.io/2025/06/22/top/</id>
    <published>2025-06-22T11:40:28.000Z</published>
    <updated>2025-06-23T17:05:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>你好呀！很高兴你能点开这个页面。这里是我的个人博客，一个用文字记录思考、分享故事的空间。如果你是第一次来访，这篇指引将帮你快速熟悉博客的「地形」，找到你感兴趣的内容。</p><h2 id="一、我会分享什么"><a href="#一、我会分享什么" class="headerlink" title="一、我会分享什么"></a>一、我会分享什么</h2><p>这个博客就像我的「数字笔记」，主要收录三大类内容：</p><h3 id="1-技术探索"><a href="#1-技术探索" class="headerlink" title="1. 技术探索"></a>1. 技术探索</h3><ul><li><p>作为一名「CSer」，这里会分享计算机科学与技术领域相关的学习笔记与技术探索</p></li><li><p>项目踩坑实录、工具效率指南</p></li><li><p>也有可能分享其他领域的技术探索</p></li></ul><h3 id="2-生活碎片"><a href="#2-生活碎片" class="headerlink" title="2. 生活碎片"></a>2. 生活碎片</h3><ul><li><p>人生旅行途中遇到的有趣灵魂与优美风景</p></li><li><p>偶尔也会发点「无用美学」</p></li></ul><h3 id="3-成长思考"><a href="#3-成长思考" class="headerlink" title="3. 成长思考"></a>3. 成长思考</h3><ul><li><p>关于自我提升、阅读感悟等的深度复盘</p></li><li><p>也会记录一些笨拙但真实的试错经历</p></li></ul><h2 id="二、如何快速找到感兴趣的内容"><a href="#二、如何快速找到感兴趣的内容" class="headerlink" title="二、如何快速找到感兴趣的内容"></a>二、如何快速找到感兴趣的内容</h2><p>如果觉得文章「眼花缭乱」，可以试试这些「导航工具」：</p><h3 id="1-菜单栏的分类与标签"><a href="#1-菜单栏的分类与标签" class="headerlink" title="1. 菜单栏的分类与标签"></a>1. 菜单栏的分类与标签</h3><ul><li><p>「分类」模块帮你按「大主题」筛选内容，点击任意分类，即可查看该类别的所有文章</p></li><li><p>「标签」模块像一个个「关键词路标」，点击标签可聚合所有相关内容</p></li></ul><h3 id="2-菜单栏的搜索功能"><a href="#2-菜单栏的搜索功能" class="headerlink" title="2. 菜单栏的搜索功能"></a>2. 菜单栏的搜索功能</h3><ul><li>「搜索」模块支持全文检索，直接输入你感兴趣的关键词，就能快速定位目标文章</li></ul><h3 id="3-菜单栏的归档部分"><a href="#3-菜单栏的归档部分" class="headerlink" title="3. 菜单栏的归档部分"></a>3. 菜单栏的归档部分</h3><ul><li>「归档」模块会简单地按照时间顺序显示所有文章，适合喜欢简洁的朋友</li></ul><h2 id="三、写在最后"><a href="#三、写在最后" class="headerlink" title="三、写在最后"></a>三、写在最后</h2><p>本站还在不断「生长」，未来可能会加入更多的功能和内容。如果你有任何建议或者想法，欢迎通过「关于」模块中的联系方式告诉我。</p><p>最后，谢谢你的停留，希望你能在这里找到一片「属于自己的文字角落」。</p>]]></content>
    
    
    <summary type="html">欢迎来到我的个人博客，如果你是第一次来访，这篇指引将帮你快速熟悉博客的「地形」，找到你感兴趣的内容。</summary>
    
    
    
    <category term="说明" scheme="https://logicff.github.io/categories/%E8%AF%B4%E6%98%8E/"/>
    
    
    <category term="说明" scheme="https://logicff.github.io/tags/%E8%AF%B4%E6%98%8E/"/>
    
  </entry>
  
  <entry>
    <title>RSS：回归主动阅读时代</title>
    <link href="https://logicff.github.io/2025/06/16/rss/"/>
    <id>https://logicff.github.io/2025/06/16/rss/</id>
    <published>2025-06-16T12:35:08.000Z</published>
    <updated>2025-06-23T12:08:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>打开各种App，“个性化”算法推送的永远是“你可能喜欢”的内容：重复的热点话题、同质化的短视频、被流量逻辑扭曲的“优质内容”。算法构建的信息茧房里，看似拥有无限选择，可实际上已经被困在“精心设计”的信息牢笼里——</p><ul><li><p>重要的内容被折叠在信息流深处</p></li><li><p>小众但高质量的内容“永远”得不到推荐</p></li><li><p>被动接收的信息碎片正在取代主动阅读</p></li></ul><p>而RSS能让信息获取回归“主动模式”，这就是为什么大家需要RSS。同时，本站已支持RSS订阅。</p><h2 id="一、什么是RSS？"><a href="#一、什么是RSS？" class="headerlink" title="一、什么是RSS？"></a>一、什么是RSS？</h2><p>RSS（简易信息聚合），即Really Simple Syndication，简单来说它是一种内容订阅协议。当网站更新内容时，会生成一个包含标题、摘要、链接的XML文件（即RSS Feed），用户可以通过RSS阅读器订阅这些Feed，就能在一个界面集中查看所有关注网站的新内容。</p><p>打个比方，就像是从一堆分散的报纸杂志中把你需要的分离出来，并统一投递到你的“数字报箱”，从此无需去各个网页走一遍，也不必担心错过关注的网站的更新。</p><h2 id="二、如何使用"><a href="#二、如何使用" class="headerlink" title="二、如何使用"></a>二、如何使用</h2><h3 id="1-找到订阅链接"><a href="#1-找到订阅链接" class="headerlink" title="1. 找到订阅链接"></a>1. 找到订阅链接</h3><p>首先在网站上找到RSS订阅的图标或链接，如果是图标的话一般可以通过右键（PC端）或长按（移动端）获取订阅链接。</p><p>以本站为例，本站的RSS链接在导航栏中，你会得到<a href="https://logicff.github.io/atom.xml">https://logicff.github.io/atom.xml</a>。将RSS地址复制下来以后，你就可以在阅读器中添加并进行订阅了。</p><h3 id="2-选择合适的RSS阅读器"><a href="#2-选择合适的RSS阅读器" class="headerlink" title="2. 选择合适的RSS阅读器"></a>2. 选择合适的RSS阅读器</h3><p>我推荐使用浏览器插件的方式，当然，还有在线网站、客户端等方式可供选择。</p><p>这里介绍浏览器插件的方式。如果你是使用Chrome，那么请打开Chrome的应用商店，搜索“RSS”即可，可以选择RSS Feed Reader或者RSS Subscription Extension（由Google提供）进行安装，也可以自行选择其他插件安装，安装完成后按照说明或者自行搜索使用方法进行使用。</p><p>其他浏览器大同小异，比如Firefox扩展中搜索“RSS”发现暂时没有上述两个插件，可以选择搜索结果中的Feedbro。</p><h3 id="3-开启RSS订阅之旅"><a href="#3-开启RSS订阅之旅" class="headerlink" title="3. 开启RSS订阅之旅"></a>3. 开启RSS订阅之旅</h3><p>将复制的RSS订阅链接导入RSS阅读器中即可。</p><h2 id="三、立刻行动"><a href="#三、立刻行动" class="headerlink" title="三、立刻行动"></a>三、立刻行动</h2><p>现在，你可以开始RSS订阅，开启主动阅读时代。信息爆炸的时代，大家更需要一片能自主耕耘的“数字田园”。RSS不仅是一个工具，也是一种对抗信息焦虑的生活方式，期待通过这种纯粹的方式，把喜欢的内容源牢牢握在手中。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1. <a href="https://www.ruanyifeng.com/blog/2006/01/rss.html">阮一峰的网络日志：如何使用RSS</a></p><p>2. <a href="https://zhuanlan.zhihu.com/p/55026716">知乎：如何用RSS订阅？</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;打开各种App，“个性化”算法推送的永远是“你可能喜欢”的内容：重复的热点话题、同质化的短视频、被流量逻辑扭曲的“优质内容”。算法构建的信息茧房里，看似拥有无限选择，可实际上已经被困在“精心设计”的信息牢笼里——&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;重要的内容被折叠在信息流深处</summary>
      
    
    
    
    <category term="信息获取" scheme="https://logicff.github.io/categories/%E4%BF%A1%E6%81%AF%E8%8E%B7%E5%8F%96/"/>
    
    
    <category term="RSS" scheme="https://logicff.github.io/tags/RSS/"/>
    
    <category term="阅读" scheme="https://logicff.github.io/tags/%E9%98%85%E8%AF%BB/"/>
    
    <category term="信息" scheme="https://logicff.github.io/tags/%E4%BF%A1%E6%81%AF/"/>
    
  </entry>
  
</feed>
